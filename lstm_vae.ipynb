{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Special vocabulary symbols:\n",
    "# PAD is used to pad a sequence to a fixed size\n",
    "# GO is for the end of the encoding\n",
    "# EOS is for the end of decoding\n",
    "# UNK is for out of vocabulary words\n",
    "_PAD, _GO, _EOS, _UNK = \"_PAD\", \"_GO\", \"_EOS\", \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "PAD_ID,GO_ID, EOS_ID, UNK_ID = range(4)\n",
    "infer_batch_size=100\n",
    "class Model(object):\n",
    "    def __init__(self, hps, gpu_mode=True, reuse=False):\n",
    "        self.gpu_mode=gpu_mode\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def _create_embedding(self,x, vocab_size, embed_size, embed_matrix=None):\n",
    "\n",
    "        # Creating an embedding matrix if one isn't given\n",
    "        if embed_matrix is None:\n",
    "            # This is a big matrix\n",
    "            embed_matrix = tf.get_variable(\n",
    "                name=\"embedding_matrix\",\n",
    "                shape=[vocab_size, embed_size],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "\n",
    "        # Perform the lookup of ids in x and perform the embedding to embed_size\n",
    "        # [batch_size, max_time, embed_size]\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, x)\n",
    "\n",
    "        return embed, embed_matrix\n",
    "\n",
    "\n",
    "    def _create_rnn_cell(self,n_neurons, n_layers, keep_prob):\n",
    "\n",
    "        import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "        cell_fw = rnn.LayerNormBasicLSTMCell(\n",
    "            num_units=n_neurons, dropout_keep_prob=keep_prob)\n",
    "        # Build deeper recurrent self.net if using more than 1 layer\n",
    "        if n_layers > 1:\n",
    "            cells = [cell_fw]\n",
    "            for layer_i in range(1, n_layers):\n",
    "                with tf.variable_scope('{}'.format(layer_i)):\n",
    "                    cell_fw = rnn.LayerNormBasicLSTMCell(\n",
    "                        num_units=n_neurons, dropout_keep_prob=keep_prob)\n",
    "                    cells.append(cell_fw)\n",
    "            cell_fw = rnn.MultiRNNCell(cells)\n",
    "        return cell_fw\n",
    "\n",
    "\n",
    "    def super_linear(self,x,\n",
    "                     output_size,\n",
    "                     scope=None,\n",
    "                     reuse=tf.AUTO_REUSE,\n",
    "                     init_w='ortho',\n",
    "                     weight_start=0.0,\n",
    "                     use_bias=True,\n",
    "                     bias_start=0.0,\n",
    "                     input_size=None):\n",
    "        \"\"\"Performs linear operation. Uses ortho init defined earlier.\"\"\"\n",
    "        shape = x.get_shape().as_list()\n",
    "        with tf.variable_scope(scope or 'linear'):\n",
    "            if reuse is True:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            w_init = None  # uniform\n",
    "            if input_size is None:\n",
    "                x_size = shape[1]\n",
    "            else:\n",
    "                x_size = input_size\n",
    "            if init_w == 'zeros':\n",
    "                w_init = tf.constant_initializer(0.0)\n",
    "            elif init_w == 'constant':\n",
    "                w_init = tf.constant_initializer(weight_start)\n",
    "            elif init_w == 'gaussian':\n",
    "                w_init = tf.random_normal_initializer(stddev=weight_start)\n",
    "            elif init_w == 'ortho':\n",
    "                w_init = lstm_ortho_initializer(1.0)\n",
    "\n",
    "            w = tf.get_variable(\n",
    "            'super_linear_w', [x_size, output_size], tf.float32, initializer=w_init)\n",
    "            if use_bias:\n",
    "                b = tf.get_variable(\n",
    "              'super_linear_b', [output_size],\n",
    "              tf.float32,\n",
    "              initializer=tf.constant_initializer(bias_start))\n",
    "                return tf.matmul(x, w) + b\n",
    "        return tf.matmul(x, w)\n",
    "    \n",
    "\n",
    "    def _create_encoder(self,embed, lengths, batch_size, n_enc_neurons, n_layers,\n",
    "                        keep_prob,z_size=512):\n",
    "\n",
    "        # Create the RNN Cells for encoder\n",
    "        with tf.variable_scope('forward'):\n",
    "            cell_fw = self._create_rnn_cell(n_enc_neurons, n_layers, keep_prob)\n",
    "\n",
    "        # Create the internal multi-layer cell for the backward RNN.\n",
    "        with tf.variable_scope('backward'):\n",
    "            cell_bw = self._create_rnn_cell(n_enc_neurons, n_layers, keep_prob)\n",
    "\n",
    "        # Now hookup the cells to the input\n",
    "        # [batch_size, max_time, embed_size]\n",
    "        (outputs, final_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_bw,\n",
    "            inputs=embed,\n",
    "            sequence_length=lengths,\n",
    "            time_major=False,\n",
    "            dtype=tf.float32)\n",
    "\n",
    "        last_state_fw, last_state_bw = final_state\n",
    "        layer_index=0\n",
    "        #if isinstance(last_state_fw[layer_index],tf.contrib.rnn.LSTMStateTuple):\n",
    "         #   encoder_state_c = tf.concat(values=(last_state_fw[layer_index].c,last_state_bw[layer_index].c),axis=1,name=\"encoder_fw_state_c\")\n",
    "         #   encoder_state_h = tf.concat(values=(last_state_fw[layer_index].h,last_state_bw[layer_index].h),axis=1,name=\"encoder_fw_state_h\")\n",
    "        #last_h_fw = cell_fw.get_output_at(last_state_fw)\n",
    "        #last_h_bw = cell_bw.get_output_at(last_state_bw)\n",
    "        encoder_state_c = tf.concat(values=(last_state_fw.c,last_state_bw.c),axis=1,name=\"encoder_fw_state_c\")\n",
    "        encoder_state_h = tf.concat(values=(last_state_fw.h,last_state_bw.h),axis=1,name=\"encoder_fw_state_h\")\n",
    "    \n",
    "        mu_h = self.super_linear(\n",
    "            encoder_state_h,\n",
    "            z_size,\n",
    "            input_size=n_enc_neurons * 2,  # bi-dir, so x2\n",
    "            scope='ENC_RNN_mu_h',\n",
    "            init_w='gaussian',\n",
    "            weight_start=0.001)\n",
    "        presig_h = self.super_linear(\n",
    "            encoder_state_h,\n",
    "            z_size,\n",
    "            input_size=n_enc_neurons * 2,  # bi-dir, so x2\n",
    "            scope='ENC_RNN_sigma_h',\n",
    "            init_w='gaussian',\n",
    "            weight_start=0.001)\n",
    "    \n",
    "    \n",
    "        sigma_h = tf.exp(presig_h / 2.0)  \n",
    "        eps_h = tf.random_normal(\n",
    "              (batch_size, z_size), 0.0, 1.0, dtype=tf.float32)\n",
    "        batch_z_h = mu_h + tf.multiply(sigma_h, eps_h)\n",
    "        sampled_z=tf.identity(batch_z_h,name=\"batch_z\")\n",
    "    \n",
    "    \n",
    "        kl_loss_z_h = -0.5 * tf.reduce_sum(\n",
    "            1.0 + 2.0 * sigma_h - tf.square(mu_h) - tf.exp(2.0 * sigma_h),\n",
    "            1)\n",
    "        \n",
    "        print(\"KL Loss shape:\")\n",
    "        print(kl_loss_z_h.get_shape())\n",
    "        \n",
    "        print(\"mu :\",mu_h.get_shape())\n",
    "        \n",
    "        enc=[]\n",
    "        print(\"batch_size:\",batch_size)\n",
    "        zero_c=tf.zeros((batch_size, z_size), dtype=tf.float32)\n",
    "        encoder_state = tf.contrib.rnn.LSTMStateTuple(c=zero_c, h=sampled_z)\n",
    "        return outputs,encoder_state,final_state[0],kl_loss_z_h,mu_h,sigma_h\n",
    "\n",
    "\n",
    "    def _create_decoder(self,cells,\n",
    "                        batch_size,\n",
    "                        encoder_outputs,\n",
    "                        encoder_state,\n",
    "                        encoder_lengths,\n",
    "                        decoding_inputs,\n",
    "                        decoding_lengths,\n",
    "                        embed_matrix,\n",
    "                        target_vocab_size,\n",
    "                        scope,\n",
    "                        max_sequence_size,use_attention=False):\n",
    "\n",
    "        from tensorflow.python.layers.core import Dense\n",
    "\n",
    "        # Output projection\n",
    "        output_layer = Dense(target_vocab_size, name='output_projection')\n",
    "\n",
    "        # Setup Attention\n",
    "        if use_attention:\n",
    "            attn_mech = tf.contrib.seq2seq.LuongAttention(\n",
    "                cells.output_size, encoder_outputs, encoder_lengths, scale=True)\n",
    "            cells = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell=cells,\n",
    "                attention_mechanism=attn_mech,\n",
    "                attention_layer_size=cells.output_size,\n",
    "                alignment_history=False)\n",
    "            initial_state = cells.zero_state(\n",
    "                dtype=tf.float32, batch_size=batch_size)\n",
    "            initial_state = initial_state.clone(cell_state=encoder_state)\n",
    "        else:\n",
    "            initial_state = encoder_state\n",
    "\n",
    "        # Setup training a build decoder\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            inputs=decoding_inputs,\n",
    "            sequence_length=decoding_lengths,\n",
    "            time_major=False)\n",
    "        #sampled_z=tf.identity(initial_state,name=\"sampled_z1\")\n",
    "        train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=cells,\n",
    "            helper=helper,\n",
    "            initial_state=initial_state,\n",
    "            output_layer=output_layer)\n",
    "        train_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            train_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=max_sequence_size)\n",
    "        train_logits = tf.identity(train_outputs.rnn_output, name='train_logits')\n",
    "\n",
    "        # Setup inference and build decoder\n",
    "        scope.reuse_variables()\n",
    "        start_tokens = tf.tile(tf.constant([GO_ID], dtype=tf.int32), [batch_size])\n",
    "        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding=embed_matrix, start_tokens=start_tokens, end_token=EOS_ID)\n",
    "        infer_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=cells,\n",
    "            helper=helper,\n",
    "            initial_state=initial_state,\n",
    "            output_layer=output_layer)\n",
    "        infer_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            infer_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=max_sequence_size)\n",
    "        infer_logits = tf.identity(infer_outputs.sample_id, name='infer_logits')\n",
    "\n",
    "        return output_layer,train_logits, infer_logits\n",
    "\n",
    "\n",
    "    def create_model(self,source_vocab_size=10000,\n",
    "                     target_vocab_size=10000,\n",
    "                     input_embed_size=512,\n",
    "                     target_embed_size=512,\n",
    "                     share_input_and_target_embedding=True,\n",
    "                     n_neurons=512,\n",
    "                     n_layers=1,\n",
    "                     use_attention=True,\n",
    "                     max_sequence_size=30):\n",
    "\n",
    "        n_enc_neurons = n_neurons\n",
    "        n_dec_neurons = n_neurons\n",
    "\n",
    "        # First sentence (i.e. input, original language sentence before translation)\n",
    "        # [batch_size, max_time]\n",
    "        source = tf.placeholder(tf.int32, shape=(None, None), name='source')\n",
    "\n",
    "        # User should also pass in the sequence lengths\n",
    "        source_lengths = tf.placeholder(\n",
    "            tf.int32, shape=(None,), name='source_lengths')\n",
    "\n",
    "        # Second sentence (i.e. reply, translation, etc...)\n",
    "        # [batch_size, max_time]\n",
    "        target = tf.placeholder(tf.int32, shape=(None, None), name='target')\n",
    "\n",
    "        # User should also pass in the sequence lengths\n",
    "        target_lengths = tf.placeholder(\n",
    "            tf.int32, shape=(None,), name='target_lengths')\n",
    "\n",
    "        # Dropout\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        # Symbolic shapes\n",
    "        batch_size, sequence_length = tf.unstack(tf.shape(source))\n",
    "\n",
    "        # Get the input to the decoder by removing last element\n",
    "        # and adding a 'go' symbol as first element\n",
    "        with tf.variable_scope('target/slicing'):\n",
    "            slice = tf.slice(target, [0, 0], [batch_size, -1])\n",
    "            decoder_input = tf.concat([tf.fill([batch_size, 1], GO_ID), slice], 1)\n",
    "\n",
    "        # Embed word ids to target embedding\n",
    "        with tf.variable_scope('source/embedding'):\n",
    "            source_embed, source_embed_matrix = self._create_embedding(\n",
    "                x=source, vocab_size=source_vocab_size, embed_size=input_embed_size)\n",
    "\n",
    "        # Embed word ids for target embedding\n",
    "        with tf.variable_scope('target/embedding'):\n",
    "            # Check if we need a new embedding matrix or not.  If we are for\n",
    "            # instance translating to another language, then we'd need different\n",
    "            # vocabularies for the input and outputs, and so new embeddings.\n",
    "            # However if we are for instance building a chatbot with the same\n",
    "            # language, then it doesn't make sense to have different embeddings and\n",
    "            # we should share them.\n",
    "            if (share_input_and_target_embedding and\n",
    "                    source_vocab_size == target_vocab_size):\n",
    "                target_input_embed, target_embed_matrix = self._create_embedding(\n",
    "                    x=decoder_input,\n",
    "                    vocab_size=target_vocab_size,\n",
    "                    embed_size=target_embed_size,\n",
    "                    embed_matrix=source_embed_matrix)\n",
    "            elif source_vocab_size != target_vocab_size:\n",
    "                raise ValueError(\n",
    "                    'source_vocab_size must equal target_vocab_size if ' +\n",
    "                    'sharing input and target embeddings')\n",
    "            else:\n",
    "                target_input_embed, target_embed_matrix = self._create_embedding(\n",
    "                    x=target,\n",
    "                    vocab_size=target_vocab_size,\n",
    "                    embed_size=target_embed_size)\n",
    "\n",
    "        # Build the encoder\n",
    "        with tf.variable_scope('encoder'):\n",
    "            #return outputs,mu_c,sigma_c,encoder_state,final_state[0],batch_z_c\n",
    "            encoder_outputs,encoder_state,final_state,kl_loss_h,mu,sigma = self._create_encoder(\n",
    "                embed=source_embed,\n",
    "                lengths=source_lengths,\n",
    "                batch_size=batch_size,\n",
    "                n_enc_neurons=n_enc_neurons,\n",
    "                n_layers=n_layers,\n",
    "                keep_prob=keep_prob)\n",
    "\n",
    "        # Build the decoder\n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "            cell_fw = self._create_rnn_cell(n_dec_neurons, n_layers, keep_prob)\n",
    "            output_layer,decoding_train_logits, decoding_infer_logits = self._create_decoder(\n",
    "                cells=cell_fw,\n",
    "                batch_size=batch_size,\n",
    "                encoder_outputs=encoder_outputs[0],\n",
    "                #encoder_state=encoder_state[0],\n",
    "                encoder_state=encoder_state,\n",
    "                encoder_lengths=source_lengths,\n",
    "                decoding_inputs=target_input_embed,\n",
    "                decoding_lengths=target_lengths,\n",
    "                embed_matrix=target_embed_matrix,\n",
    "                target_vocab_size=target_vocab_size,\n",
    "                scope=scope,\n",
    "                max_sequence_size=max_sequence_size)\n",
    "            #cell_tensor=tf.identity(cell_fw,name=\"saved_cell\")\n",
    "        with tf.variable_scope('loss'):\n",
    "            weights = tf.cast(tf.sequence_mask(target_lengths), tf.float32)\n",
    "            kl_loss = tf.reduce_mean(kl_loss_h)\n",
    "            loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=tf.reshape(decoding_train_logits, [\n",
    "                    batch_size, tf.reduce_max(target_lengths), target_vocab_size\n",
    "                ]),\n",
    "                targets=target,\n",
    "                weights=weights) + tf.reduce_mean(kl_loss_h)\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'source': source,\n",
    "            'source_lengths': source_lengths,\n",
    "            'target': target,\n",
    "            'target_lengths': target_lengths,\n",
    "            'keep_prob': keep_prob,\n",
    "            'embedding_matrix':source_embed_matrix,\n",
    "            'thought_vector': encoder_state,\n",
    "            'decoder': decoding_infer_logits,\n",
    "            'decoder_cells':cell_fw,\n",
    "            'output_layer':output_layer,\n",
    "            'prob_state':encoder_state,\n",
    "            'final_state':final_state,\n",
    "            'kl_loss':kl_loss,\n",
    "            'mu':mu,\n",
    "            'sigma':sigma\n",
    "            \n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "    def batch_generator(self,sources,\n",
    "                        targets,\n",
    "                        source_lengths,\n",
    "                        target_lengths,\n",
    "                        batch_size=10):\n",
    "\n",
    "        idxs = np.random.permutation(np.arange(len(sources)))\n",
    "        n_batches = len(idxs) // batch_size\n",
    "        for batch_i in range(n_batches):\n",
    "            this_idxs = idxs[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "            this_sources, this_targets = sources[this_idxs, :], targets[\n",
    "                this_idxs, :]\n",
    "            this_source_lengths, this_target_lengths = source_lengths[\n",
    "                this_idxs], target_lengths[this_idxs]\n",
    "            yield (this_sources[:, :np.max(this_source_lengths)],\n",
    "                   this_targets[:, :np.max(this_target_lengths)],\n",
    "                   this_source_lengths, this_target_lengths)\n",
    "\n",
    "\n",
    "    def preprocess(self,text,vocab=None, min_count=5, min_length=3, max_length=30):\n",
    "\n",
    "        sentences = [el for s in text for el in nltk.sent_tokenize(s)]\n",
    "    \n",
    "        # We'll first tokenize each sentence into words to get a sense of\n",
    "        # how long each sentence is:\n",
    "        words = [[word.lower() for word in nltk.word_tokenize(s)]\n",
    "                 for s in sentences]\n",
    "\n",
    "        # Then see how long each sentence is:\n",
    "        lengths = np.array([len(s) for s in words])\n",
    "\n",
    "        good_idxs = np.where((lengths >= min_length) & (lengths < max_length))[0]\n",
    "        dataset = [words[idx] for idx in good_idxs]\n",
    "        fdist = nltk.FreqDist([word for sentence in dataset for word in sentence])\n",
    "\n",
    "        vocab_counts = [el for el in fdist.most_common() if el[1] > min_count]\n",
    "        if vocab is None:\n",
    "        # First sort the vocabulary\n",
    "            vocab = [v[0] for v in vocab_counts]\n",
    "            vocab.sort()\n",
    "\n",
    "            # Now add the special symbols:\n",
    "            vocab = _START_VOCAB + vocab\n",
    "\n",
    "            # Then create the word to id mapping\n",
    "            vocab = {k: v for v, k in enumerate(vocab)}\n",
    "\n",
    "            with open('vocab.pkl', 'wb') as fp:\n",
    "                pickle.dump(vocab, fp)\n",
    "\n",
    "        unked = self.word2id(dataset, vocab)\n",
    "        return unked, vocab\n",
    "\n",
    "\n",
    "    def word2id(self,words, vocab):\n",
    "\n",
    "        unked = []\n",
    "        for s in words:\n",
    "            this_sentence = [vocab.get(w, UNK_ID) for w in s]\n",
    "            unked.append(this_sentence)\n",
    "        return unked\n",
    "\n",
    "\n",
    "    def id2word(self,ids, vocab):\n",
    "\n",
    "        words = []\n",
    "        id2words = {v: k for k, v in vocab.items()}\n",
    "        for s in ids:\n",
    "            this_sentence = [id2words.get(w) for w in s]\n",
    "            words.append(this_sentence)\n",
    "        return words\n",
    "\n",
    "    def decode(self,sess,tokens, lengths):\n",
    "        decoding = sess.run(\n",
    "            self.net['decoder'],\n",
    "            feed_dict={\n",
    "                self.net['keep_prob']: 1.0,\n",
    "                self.net['source']: tokens,\n",
    "                self.net['source_lengths']: lengths\n",
    "            })\n",
    "        print('input:', \" \".join(self.id2word([tokens[0]], self.vocab)[0]))\n",
    "        print('output:', \" \".join(self.id2word([decoding[0]], self.vocab)[0]))\n",
    "        print('ouput codes:',decoding[0])\n",
    "            \n",
    "    def train(self,text,\n",
    "              max_sequence_size=20,\n",
    "              use_attention=False,\n",
    "              min_count=0,\n",
    "              min_length=5,\n",
    "              n_epochs=1000,\n",
    "              batch_size=50,z_size=512):\n",
    "\n",
    "        # Preprocess it to word IDs including UNKs for out of vocabulary words\n",
    "        self.unked, self.vocab = self.preprocess(\n",
    "            text,\n",
    "            min_count=min_count,\n",
    "            min_length=min_length,\n",
    "            max_length=max_sequence_size - 1)\n",
    "\n",
    "        # Get the vocabulary size\n",
    "        vocab_size = len(self.vocab)\n",
    "        print(vocab_size)\n",
    "        # Create input output pairs formed by neighboring sentences of dialog\n",
    "        #sources_list, targets_list = unked[:-1], unked[1:]\n",
    "        sources_list = self.unked\n",
    "        targets_list = self.unked\n",
    "        # Store the final lengths\n",
    "        source_lengths = np.zeros((len(sources_list)), dtype=np.int32)\n",
    "        target_lengths = np.zeros((len(targets_list)), dtype=np.int32)\n",
    "        sources = np.ones(\n",
    "            (len(sources_list), max_sequence_size), dtype=np.int32) * PAD_ID\n",
    "        targets = np.ones(\n",
    "            (len(targets_list), max_sequence_size), dtype=np.int32) * PAD_ID\n",
    "\n",
    "        for i, (source_i, target_i) in enumerate(zip(sources_list, targets_list)):\n",
    "            el = source_i\n",
    "            source_lengths[i] = len(el)\n",
    "            sources[i, :len(el)] = el\n",
    "\n",
    "            el = target_i + [EOS_ID]\n",
    "            target_lengths[i] = len(el)\n",
    "            targets[i, :len(el)] = el\n",
    "\n",
    "        sess = tf.Session()\n",
    "\n",
    "        self.net = self.create_model(\n",
    "            max_sequence_size=max_sequence_size,\n",
    "            use_attention=use_attention,\n",
    "            source_vocab_size=vocab_size,\n",
    "            target_vocab_size=vocab_size)\n",
    "\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.net['loss'])\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        #output_layer = Dense(vocab_size, name='output_projection_prediction')\n",
    "        infer_cells_ = self.net['decoder_cells']\n",
    "        infer_output_layer_ = self.net['output_layer']\n",
    "        start_tokens_ = tf.tile(tf.constant([GO_ID], dtype=tf.int32), [infer_batch_size])\n",
    "        zero_c=tf.zeros((infer_batch_size, z_size), dtype=tf.float32)\n",
    "        z=tf.placeholder(tf.float32, shape=(infer_batch_size, z_size), name='z_sampled')\n",
    "        infer_encoder_state_ = tf.contrib.rnn.LSTMStateTuple(c=zero_c, h=z)\n",
    "        infer_helper_ = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding=self.net['embedding_matrix'], start_tokens=start_tokens_, end_token=EOS_ID)\n",
    "        infer_decoder_ = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=infer_cells_,\n",
    "                helper=infer_helper_,\n",
    "                initial_state=infer_encoder_state_,\n",
    "                output_layer=infer_output_layer_)\n",
    "        infer_outputs_, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                infer_decoder_,\n",
    "                output_time_major=False,\n",
    "                impute_finished=True,\n",
    "                maximum_iterations=max_sequence_size)\n",
    "        \n",
    "        infer_logits_ = tf.identity(infer_outputs_.sample_id, name='infer_logits_pred')\n",
    "        sess.run(init_op)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "\n",
    "        current_learning_rate = 0.01\n",
    "        epoch_i=0\n",
    "        test_sent=[]\n",
    "        test_sent_len=0\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "            #for epoch_i in range(n_epochs):\n",
    "                total = 0\n",
    "                for it_i, (this_sources, this_targets, this_source_lengths, this_target_lengths) \\\n",
    "                    in enumerate(self.batch_generator(\n",
    "                        sources, targets, source_lengths, target_lengths, batch_size=batch_size)):\n",
    "                    if it_i % 1000 == 0:\n",
    "                        current_learning_rate = max(0.0001,\n",
    "                                                    current_learning_rate * 0.99)\n",
    "                        if(epoch_i==1):\n",
    "                            test_sent=this_sources[0:1]\n",
    "                            test_sent_len=this_source_lengths[0:1]\n",
    "                        self.decode(sess,this_sources[0:1], this_source_lengths[0:1])\n",
    "                    l,kl,_ = sess.run(\n",
    "                        [self.net['loss'],self.net['kl_loss'], opt],\n",
    "                        feed_dict={\n",
    "                            learning_rate: current_learning_rate,\n",
    "                            self.net['keep_prob']: 0.8,\n",
    "                            self.net['source']: this_sources,\n",
    "                            self.net['target']: this_targets,\n",
    "                            self.net['source_lengths']: this_source_lengths,\n",
    "                            self.net['target_lengths']: this_target_lengths\n",
    "                        })\n",
    "\n",
    "                    prob_state,final_state,mu_,sigma_=sess.run([self.net['prob_state'],self.net['final_state'],self.net['mu'],self.net['sigma']],feed_dict={\n",
    "                            learning_rate: current_learning_rate,\n",
    "                            self.net['keep_prob']: 0.8,\n",
    "                            self.net['source']: this_sources,\n",
    "                            self.net['target']: this_targets,\n",
    "                            self.net['source_lengths']: this_source_lengths,\n",
    "                            self.net['target_lengths']: this_target_lengths\n",
    "                        })\n",
    "\n",
    "                    total = total + l\n",
    "                    print('{}: {}'.format(it_i, total / (it_i + 1)), end='\\r')\n",
    "                    print(\"KL Loss:\",kl)\n",
    "                # End of epoch, save\n",
    "                    print('epoch {}: {}'.format(epoch_i, total / it_i))\n",
    "                epoch_i=epoch_i+1\n",
    "                if epoch_i==10:\n",
    "                    coord.request_stop()\n",
    "                                \n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # One of the threads has issued an exception.  So let's tell all the\n",
    "            # threads to shutdown.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait until all threads have finished.\n",
    "        coord.join(threads)\n",
    "        saver.save(sess, './vae-rnn.ckpt', global_step=epoch_i)\n",
    "        def slerp(p0, p1, t):\n",
    "            omega = np.arccos(np.dot(p0 / np.linalg.norm(p0), p1 / np.linalg.norm(p1)))\n",
    "            so = np.sin(omega)\n",
    "            return np.sin((1.0 - t) * omega) / so * p0 + np.sin(t * omega) / so * p1\n",
    "    \n",
    "        def get_thought_vector(txt,txt_len):\n",
    "            thgt_vect=sess.run(self.net['thought_vector'],feed_dict={self.net['keep_prob']: 1,self.net['source']: txt,\n",
    "                        self.net['source_lengths']: txt_len})\n",
    "            return thgt_vect\n",
    "    \n",
    "\n",
    "        print(\"mu shape:\",mu_)\n",
    "        print(\"sigma:\",sigma_)\n",
    "        z_1 = np.random.randn(infer_batch_size,512)\n",
    "        eps_h = np.random.randn(infer_batch_size, z_size)\n",
    "        #z_1 = mu_[0:infer_batch_size,] + np.multiply(sigma_[0:infer_batch_size,], eps_h)\n",
    "        predictions=sess.run(infer_logits_,feed_dict={z:z_1,self.net['keep_prob']:1.0})\n",
    "       \n",
    "    \n",
    "        print(\"Test Sents:\")\n",
    "        for i in range(infer_batch_size):\n",
    "            print('output:', \" \".join(self.id2word([predictions[i]], self.vocab)[0]))\n",
    "            #print(\"Z_1:\",z_1[i,:])\n",
    "        N = 10\n",
    "        for t in np.linspace(0, 1, N):\n",
    "            print(\"Interpolation step:\", int(t*10))\n",
    "            #z_i=slerp(z_0[0][0], z_1[0][0], t)\n",
    "            z_i=z_0[1][0] + (1-t)*(z_1[1][0]-z_0[1][0])\n",
    "            #z_h=z_i[np.newaxis,]\n",
    "            zero_c=np.zeros((1, 512), dtype=np.float32)\n",
    "\n",
    "            #new_enc_state = np.concatenate([zero_c,z_i[np.newaxis,]],0)\n",
    "            new_enc_state =tf.contrib.rnn.LSTMStateTuple(c=zero_c,h=z_i[np.newaxis,])\n",
    "            #print(id2word(test_sent, vocab)[0])\n",
    "            decod=sess.run(self.net['decoder'],feed_dict={self.net['keep_prob']: 1,self.net['thought_vector']:new_enc_state,self.net['source']: test_sent,\n",
    "                        self.net['source_lengths']: test_sent_len})\n",
    "            print('decoder output:', self.id2word(decod, self.vocab)[0])\n",
    "   \n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "KL Loss shape:\n",
      "(?,)\n",
      "mu : (?, 512)\n",
      "batch_size: Tensor(\"unstack:0\", shape=(), dtype=int32)\n",
      "input: we expect reprisals from china as well as an onslaught of criticism . _PAD _PAD _PAD _PAD _PAD\n",
      "output: specifically his likely his 20report_chinese_capabilitiesforcomputer_networkoperationsandcyberespionage.pdf 20report_chinese_capabilitiesforcomputer_networkoperationsandcyberespionage.pdf configured publishing 141 conservatively published shows shows other other net confirmation net china while\n",
      "ouput codes: [279 153 187 153  23  23 101 245  15 104 244 271 271 226 226 202 102 202\n",
      "  87 323]\n",
      "KL Loss: 1123.7645375\n",
      "epoch 0: inf\n",
      "input: apt1 focuses on compromising organizations across a broad range of industries in english-speaking countries . _PAD _PAD _PAD\n",
      "output: » » » » » , _EOS\n",
      "ouput codes: [330 330 330 330 330   7   2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:568: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Loss: 508.9717725\n",
      "epoch 1: inf\n",
      "input: we expect reprisals from china as well as an onslaught of criticism . _PAD _PAD _PAD _PAD _PAD\n",
      "output: we , the , _EOS\n",
      "ouput codes: [319   7 291   7   2]\n",
      "KL Loss: 410.5163219\n",
      "epoch 2: inf\n",
      "input: the decision to publish a significant part of our intelligence about unit 61398 was a painstaking one _PAD\n",
      "output: » » » » » » » » » » » » » » » » » » » »\n",
      "ouput codes: [330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330\n",
      " 330 330]\n",
      "KL Loss: 184.32288844\n",
      "epoch 3: inf\n",
      "input: 3 our conclusions are based exclusively on unclassified , open source information derived from mandiant observations _PAD _PAD\n",
      "output: » » » » » » » » » » » » » » » » » » » »\n",
      "ouput codes: [330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330\n",
      " 330 330]\n",
      "KL Loss: 248.1564603\n",
      "epoch 4: inf\n",
      "input: none of the information in this report involves access to or confirmation by classified intelligence . _PAD _PAD\n",
      "output: » » » » » » » » » » » » » » » » » » » »\n",
      "ouput codes: [330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330\n",
      " 330 330]\n",
      "KL Loss: 121.65539438\n",
      "epoch 5: inf\n",
      "input: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _PAD _PAD _PAD\n",
      "output: » » » » » » » » » » » » » » » » » » » »\n",
      "ouput codes: [330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330 330\n",
      " 330 330]\n",
      "KL Loss: 89.01796328\n",
      "epoch 6: inf\n",
      "input: 1 , 2013 , http : //www.washingtonpost.com/business/ technology/chinese-hackers-suspected-in-attack-on-the-posts-computers/2013/02/01/d5a44fde-6cb1-11e2-bd36-c0fe61a205f6_story.html , accessed feb _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "output: » » » » specifically , mandiant is providing the following : _EOS\n",
      "ouput codes: [330 330 330 330 279   7 194 174 241 291 140  35   2]\n",
      "KL Loss: 108.74334406\n",
      "epoch 7: inf\n",
      "input: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _PAD _PAD _PAD\n",
      "output: » » » specifically , mandiant is providing the following : _EOS\n",
      "ouput codes: [330 330 330 279   7 194 174 241 291 140  35   2]\n",
      "KL Loss: 103.36327344\n",
      "epoch 8: inf\n",
      "input: mandiant is releasing more than 3,000 indicators to bolster defenses against apt1 operations . _PAD _PAD _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS\n",
      "ouput codes: [295   7 194 174 129 291 189  76 211  56  42 160 298 252   8   2]\n",
      "KL Loss: 73.99294344\n",
      "epoch 9: inf\n",
      "mu shape: [[-0.32343993 -0.13777746 -0.49537992 ...  0.09849142  0.42329076\n",
      "   0.9212845 ]\n",
      " [-0.31440252 -0.1181509  -0.5024383  ...  0.09098317  0.45101196\n",
      "   0.92404   ]\n",
      " [-0.33748546 -0.18012738 -0.53653175 ...  0.09526797  0.4346772\n",
      "   0.9008474 ]\n",
      " ...\n",
      " [-0.2977811  -0.15278576 -0.5013163  ...  0.11508265  0.37196907\n",
      "   0.8786271 ]\n",
      " [-0.37338707 -0.18027756 -0.47274214 ...  0.16096555  0.41117123\n",
      "   0.9313083 ]\n",
      " [-0.38366342 -0.20448928 -0.57687324 ...  0.13870226  0.34089822\n",
      "   1.0185919 ]]\n",
      "sigma: [[0.00096793 0.00094369 0.00097406 ... 0.00096267 0.00095067 0.00095803]\n",
      " [0.00093305 0.00091085 0.00094186 ... 0.00092851 0.0009163  0.00092305]\n",
      " [0.00108102 0.00105337 0.00108801 ... 0.001074   0.00106172 0.00106823]\n",
      " ...\n",
      " [0.00107352 0.00104965 0.0010855  ... 0.00107045 0.00105975 0.00106045]\n",
      " [0.00102698 0.00100309 0.00103487 ... 0.00102053 0.00101032 0.00101734]\n",
      " [0.0009338  0.00091097 0.00094103 ... 0.00092605 0.00091677 0.00092295]]\n",
      "Test Sents:\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , the 2nd bureau also sits atop a large-scale organization of subordinate offices . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: therefore , mandiant is establishing the lower bounds of apt1 activities in this report . _EOS _PAD _PAD\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "output: − » 613 ( 99.8 % ) were registered to one of four shanghai net blocks . _EOS\n",
      "Interpolation step: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'z_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-85321e45d7fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\Vinod\\projects\\\\sentence-autoencoder\\\\China_hack.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-d51fc38cfee2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, text, max_sequence_size, use_attention, min_count, min_length, n_epochs, batch_size, z_size)\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interpolation step:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[1;31m#z_i=slerp(z_0[0][0], z_1[0][0], t)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m             \u001b[0mz_i\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mz_0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mz_0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m             \u001b[1;31m#z_h=z_i[np.newaxis,]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[0mzero_c\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'z_0' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import codecs\n",
    "def read_lines(fname):\n",
    "    ret=[]\n",
    "    indx=-1\n",
    "    for line in codecs.open(fname, encoding=\"utf-8\"):\n",
    "        sent = line\n",
    "        indx=indx+1\n",
    "        ret.append(sent)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "md=Model(hps=None)\n",
    "txt=read_lines('C:\\\\Users\\\\Vinod\\projects\\\\sentence-autoencoder\\\\China_hack.txt')\n",
    "md.train(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder/infer_logits:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./vae-rnn.ckpt-10\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value decoder/layer_norm_basic_lstm_cell/state/beta\n\t [[Node: decoder/layer_norm_basic_lstm_cell/state/beta/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](decoder/layer_norm_basic_lstm_cell/state/beta)]]\n\t [[Node: decoder/while/BasicDecoderStep/ArgMax/_289 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_249_decoder/while/BasicDecoderStep/ArgMax\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopdecoder/while/Select_2/_247)]]\n\nCaused by op 'decoder/layer_norm_basic_lstm_cell/state/beta/read', defined at:\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-36-919c7c1436e9>\", line 53, in <module>\n    maximum_iterations=max_sequence_size)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 309, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2934, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2720, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2662, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 254, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\basic_decoder.py\", line 138, in step\n    cell_outputs, cell_state = self._cell(inputs, state)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 188, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 652, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn_cell.py\", line 1414, in call\n    new_c = self._norm(new_c, \"state\", dtype=dtype)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn_cell.py\", line 1378, in _norm\n    vs.get_variable(\"beta\", shape=shape, initializer=beta_init, dtype=dtype)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1262, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1097, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 427, in get_variable\n    return custom_getter(**custom_getter_kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 191, in _rnn_get_variable\n    variable = getter(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 404, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 806, in _get_single_variable\n    constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 229, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 373, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 127, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2728, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value decoder/layer_norm_basic_lstm_cell/state/beta\n\t [[Node: decoder/layer_norm_basic_lstm_cell/state/beta/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](decoder/layer_norm_basic_lstm_cell/state/beta)]]\n\t [[Node: decoder/while/BasicDecoderStep/ArgMax/_289 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_249_decoder/while/BasicDecoderStep/ArgMax\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopdecoder/while/Select_2/_247)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value decoder/layer_norm_basic_lstm_cell/state/beta\n\t [[Node: decoder/layer_norm_basic_lstm_cell/state/beta/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](decoder/layer_norm_basic_lstm_cell/state/beta)]]\n\t [[Node: decoder/while/BasicDecoderStep/ArgMax/_289 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_249_decoder/while/BasicDecoderStep/ArgMax\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopdecoder/while/Select_2/_247)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-919c7c1436e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mimpute_finished\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             maximum_iterations=max_sequence_size)\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0minfer_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfer_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mz_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;31m#infer_logits=sess.run(logits,{input_z: z_1,source:[unked]*batch_size,source_lengths:[source_length]*batch_size,keep_prob:1.0} )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m#infer_logits=sess.run(logits,{input_z: z_1,embedding_matrix:embed})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value decoder/layer_norm_basic_lstm_cell/state/beta\n\t [[Node: decoder/layer_norm_basic_lstm_cell/state/beta/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](decoder/layer_norm_basic_lstm_cell/state/beta)]]\n\t [[Node: decoder/while/BasicDecoderStep/ArgMax/_289 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_249_decoder/while/BasicDecoderStep/ArgMax\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopdecoder/while/Select_2/_247)]]\n\nCaused by op 'decoder/layer_norm_basic_lstm_cell/state/beta/read', defined at:\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-36-919c7c1436e9>\", line 53, in <module>\n    maximum_iterations=max_sequence_size)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 309, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2934, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2720, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2662, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 254, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\basic_decoder.py\", line 138, in step\n    cell_outputs, cell_state = self._cell(inputs, state)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 188, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 652, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn_cell.py\", line 1414, in call\n    new_c = self._norm(new_c, \"state\", dtype=dtype)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn_cell.py\", line 1378, in _norm\n    vs.get_variable(\"beta\", shape=shape, initializer=beta_init, dtype=dtype)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1262, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1097, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 427, in get_variable\n    return custom_getter(**custom_getter_kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 191, in _rnn_get_variable\n    variable = getter(*args, **kwargs)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 404, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 806, in _get_single_variable\n    constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 229, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 373, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 127, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2728, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value decoder/layer_norm_basic_lstm_cell/state/beta\n\t [[Node: decoder/layer_norm_basic_lstm_cell/state/beta/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](decoder/layer_norm_basic_lstm_cell/state/beta)]]\n\t [[Node: decoder/while/BasicDecoderStep/ArgMax/_289 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_249_decoder/while/BasicDecoderStep/ArgMax\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopdecoder/while/Select_2/_247)]]\n"
     ]
    }
   ],
   "source": [
    "batch_size=10\n",
    "embed_size=512\n",
    "z_size=512\n",
    "n_dec_neurons = 512\n",
    "n_layers = 1\n",
    "keep_prob = 1.0\n",
    "max_sequence_size = 20\n",
    "from tensorflow.python.layers.core import Dense\n",
    "with open('vocab.pkl', 'rb') as fp:\n",
    "    vocab=pickle.load(fp)\n",
    "    \n",
    "source = [\"the apt uses\"]\n",
    "source_length=[1]\n",
    "z_1 = np.random.randn(batch_size,512)\n",
    "\n",
    "checkpoint = \"./vae-rnn.ckpt-10\"\n",
    "model=Model(hps=None)\n",
    "unked,_=model.preprocess(source,vocab)\n",
    "vocab_size=len(vocab)\n",
    "loaded_graph = tf.Graph()\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_z = loaded_graph.get_tensor_by_name('encoder/batch_z:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('decoder/infer_logits:0')\n",
    "    source = loaded_graph.get_tensor_by_name('source:0')\n",
    "    source_lengths = loaded_graph.get_tensor_by_name('source_lengths:0')\n",
    "    keep_prob=loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    embedding_matrix=loaded_graph.get_tensor_by_name('source/embedding/embedding_matrix:0')\n",
    "    embed=embedding_matrix.eval()\n",
    "    \n",
    "#working_graph=tf.Graph()\n",
    "#with tf.Session(graph=working_graph) as sess:\n",
    "    output_layer = Dense(vocab_size, name='output_projection')\n",
    "    cells = model._create_rnn_cell(n_dec_neurons, n_layers, keep_prob)\n",
    "    start_tokens = tf.tile(tf.constant([GO_ID], dtype=tf.int32), [batch_size])\n",
    "    zero_c=tf.zeros((batch_size, z_size), dtype=tf.float32)\n",
    "    z=tf.placeholder(tf.float32, shape=(batch_size, z_size), name='z_sampled')\n",
    "    encoder_state = tf.contrib.rnn.LSTMStateTuple(c=zero_c, h=z)\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding=embedding_matrix, start_tokens=start_tokens, end_token=EOS_ID)\n",
    "    infer_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=cells,\n",
    "            helper=helper,\n",
    "            initial_state=encoder_state,\n",
    "            output_layer=output_layer)\n",
    "    infer_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            infer_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=max_sequence_size)\n",
    "    infer_logits=sess.run(infer_outputs,feed_dict={z:z_1,keep_prob:1.0})\n",
    "    #infer_logits=sess.run(logits,{input_z: z_1,source:[unked]*batch_size,source_lengths:[source_length]*batch_size,keep_prob:1.0} )\n",
    "    #infer_logits=sess.run(logits,{input_z: z_1,embedding_matrix:embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-92e1e193e08d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0munked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munked\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \"\"\"\n\u001b[1;32m--> 648\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4739\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4740\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4741\u001b[1;33m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[0;32m   4742\u001b[0m                        \u001b[1;34m\"session is registered. Use `with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4743\u001b[0m                        \u001b[1;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "source = [\"the apt uses\"]\n",
    "unked,_=model.preprocess(source,vocab)\n",
    "([unked]*10)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4826095 , -0.29743624,  0.9528408 , ..., -0.27740264,\n",
       "         0.9505074 , -0.3581655 ],\n",
       "       [-0.47100595, -0.1888142 , -0.37042865, ..., -0.2040554 ,\n",
       "        -0.8872597 , -0.5737    ],\n",
       "       [-0.7269716 ,  0.15058541,  0.18357754, ...,  0.30755734,\n",
       "        -0.55266976,  0.16425014],\n",
       "       ...,\n",
       "       [ 0.32169425, -0.27367222, -0.71774787, ..., -0.8827288 ,\n",
       "        -0.46514425, -0.09313238],\n",
       "       [ 0.9734705 ,  0.5444694 , -0.56080586, ...,  0.21127783,\n",
       "        -0.02388286,  0.5769358 ],\n",
       "       [-0.1442375 , -0.02128563,  0.8911779 , ...,  0.2750093 ,\n",
       "        -0.68218243,  0.9351503 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder/ENC_RNN_mu_h/super_linear_w/Initializer/random_normal/shape',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Initializer/random_normal/mean',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Initializer/random_normal/stddev',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Initializer/random_normal/RandomStandardNormal',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Initializer/random_normal/mul',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Initializer/random_normal',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Assign',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/read',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Initializer/Const',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Assign',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/read',\n",
       " 'encoder/ENC_RNN_mu_h/MatMul',\n",
       " 'encoder/ENC_RNN_mu_h/add',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/Shape',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/Shape_1',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/BroadcastGradientArgs',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/Sum',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/Reshape',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/Sum_1',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/Reshape_1',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/tuple/group_deps',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/tuple/control_dependency',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/add_grad/tuple/control_dependency_1',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/MatMul_grad/MatMul',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/MatMul_grad/MatMul_1',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/MatMul_grad/tuple/group_deps',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/MatMul_grad/tuple/control_dependency',\n",
       " 'gradients/encoder/ENC_RNN_mu_h/MatMul_grad/tuple/control_dependency_1',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Adam/Initializer/zeros',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Adam',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Adam/Assign',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Adam/read',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Adam_1/Initializer/zeros',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Adam_1',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Adam_1/Assign',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_w/Adam_1/read',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Adam/Initializer/zeros',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Adam',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Adam/Assign',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Adam/read',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Adam_1/Initializer/zeros',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Adam_1',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Adam_1/Assign',\n",
       " 'encoder/ENC_RNN_mu_h/super_linear_b/Adam_1/read',\n",
       " 'Adam/update_encoder/ENC_RNN_mu_h/super_linear_w/ApplyAdam',\n",
       " 'Adam/update_encoder/ENC_RNN_mu_h/super_linear_b/ApplyAdam']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n.name for n in loaded_graph.as_graph_def().node if \"ENC_RNN_mu_h\" in n.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parents(op):\n",
    "    return set(input.op for input in op.inputs)\n",
    "\n",
    "def children(op):\n",
    "    return set(op for out in op.outputs for op in out.consumers())\n",
    "\n",
    "def get_graph():\n",
    "    \"\"\"Creates dictionary {node: {child1, child2, ..},..} for current\n",
    "    TensorFlow graph. Result is compatible with networkx/toposort\"\"\"\n",
    "\n",
    "    ops = loaded_graph.get_operations()\n",
    "    return {op: children(op) for op in ops}\n",
    "\n",
    "\n",
    "def print_tf_graph(graph):\n",
    "    \"\"\"Prints tensorflow graph in dictionary form.\"\"\"\n",
    "    for node in graph:\n",
    "        for child in graph[node]:\n",
    "            print(\"%s -> %s\" % (node.name, child.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.FileWriter(\"logs\", loaded_graph).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
