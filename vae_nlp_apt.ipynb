{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Vinod\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy.stats import norm\n",
    "import nltk.data\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import reuters\n",
    "from nltk. corpus import gutenberg\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing code is data specific.  \n",
    "  \n",
    "It is an example of how one can use a pre-trained word2vec to embed sentences into a vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_sent (text):\n",
    "    strg = ''\n",
    "    for word in text:\n",
    "        strg += word\n",
    "        strg += ' '\n",
    "    strg_cleaned = strg.lower()\n",
    "    for x in ['\\xd5d','\\n','\"',\"!\", '#','$','%','&','(',')','*','+',',','-','/',':',';','<','=','>','?','@','[','^',']','_','`','{','|','}','~','\\t']:\n",
    "        strg_cleaned = strg_cleaned.replace(x, '')\n",
    "    sentences = sent_tokenize(strg_cleaned)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentences):\n",
    "    vectorized = []\n",
    "    for sentence in sentences:\n",
    "        byword = sentence.split()\n",
    "        concat_vector = []\n",
    "        for word in byword:\n",
    "            try:\n",
    "                concat_vector.append(w2v[word])\n",
    "            except:\n",
    "                pass\n",
    "        vectorized.append(concat_vector)\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing text from a variety of different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "data_concat = []\n",
    "all_text=[]\n",
    "txt_indx=0\n",
    "\n",
    "\n",
    "#text = split_into_sent(brown.words())\n",
    "\n",
    "\n",
    "    \n",
    "model = fasttext.skipgram(\"APT_sanitized.txt\", 'apt_sanitized',dim=100, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('apt_vectors.vec')\n",
    "data_concat = []\n",
    "\n",
    "\n",
    "with open('APT_sanitized.txt',\"r\",encoding='utf-8') as f:\n",
    "    text=f.readlines()\n",
    "vect = vectorize_sentences(text)\n",
    "data = [x for x in vect  if len(x) == 10]\n",
    "for x in data:\n",
    "    data_concat.append(list(itertools.chain.from_iterable(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to shuffle the text vectors before splitting them into test and train samples.   \n",
    "  \n",
    "This is done to avoid clumping text with similar context and style in the dataset because it can confuse the neural network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00227 , -0.002735,  0.004701, -0.004814, -0.001183, -0.002806,\n",
       "        0.000746,  0.004251, -0.004267, -0.000771, -0.011192, -0.001916,\n",
       "        0.003505,  0.003675,  0.006862, -0.00515 , -0.005831, -0.00332 ,\n",
       "        0.009895, -0.004621,  0.005789, -0.001071,  0.002866, -0.006613,\n",
       "       -0.000891, -0.006907,  0.007527,  0.005952, -0.00475 , -0.001488,\n",
       "       -0.005064,  0.010672,  0.005715, -0.001045, -0.009741,  0.001058,\n",
       "       -0.002325, -0.001972,  0.000217, -0.000577,  0.00537 ,  0.000388,\n",
       "        0.001798,  0.003835, -0.001138, -0.001426,  0.003441,  0.007415,\n",
       "       -0.000421, -0.007193, -0.002982, -0.000531, -0.003634, -0.002289,\n",
       "       -0.000626,  0.004089, -0.001688,  0.002588,  0.004141,  0.010842,\n",
       "        0.00107 ,  0.006035, -0.005503,  0.016965, -0.001544,  0.005332,\n",
       "        0.004329, -0.002427,  0.009539,  0.001806,  0.002386,  0.001555,\n",
       "       -0.000354,  0.001823, -0.003668,  0.006647,  0.000238,  0.006995,\n",
       "       -0.002169,  0.000718, -0.000288, -0.001508,  0.00395 , -0.000166,\n",
       "       -0.009835,  0.008755, -0.001973, -0.001789, -0.001014, -0.009136,\n",
       "       -0.006873, -0.00546 , -0.013583, -0.00226 , -0.005945,  0.002665,\n",
       "        0.00933 ,  0.004062, -0.006291,  0.00214 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00227,\n",
       " -0.002735,\n",
       " 0.004701,\n",
       " -0.004814,\n",
       " -0.001183,\n",
       " -0.002806,\n",
       " 0.000746,\n",
       " 0.004251,\n",
       " -0.004267,\n",
       " -0.000771,\n",
       " -0.011192,\n",
       " -0.001916,\n",
       " 0.003505,\n",
       " 0.003675,\n",
       " 0.006862,\n",
       " -0.00515,\n",
       " -0.005831,\n",
       " -0.00332,\n",
       " 0.009895,\n",
       " -0.004621,\n",
       " 0.005789,\n",
       " -0.001071,\n",
       " 0.002866,\n",
       " -0.006613,\n",
       " -0.000891,\n",
       " -0.006907,\n",
       " 0.007527,\n",
       " 0.005952,\n",
       " -0.00475,\n",
       " -0.001488,\n",
       " -0.005064,\n",
       " 0.010672,\n",
       " 0.005715,\n",
       " -0.001045,\n",
       " -0.009741,\n",
       " 0.001058,\n",
       " -0.002325,\n",
       " -0.001972,\n",
       " 0.000217,\n",
       " -0.000577,\n",
       " 0.00537,\n",
       " 0.000388,\n",
       " 0.001798,\n",
       " 0.003835,\n",
       " -0.001138,\n",
       " -0.001426,\n",
       " 0.003441,\n",
       " 0.007415,\n",
       " -0.000421,\n",
       " -0.007193,\n",
       " -0.002982,\n",
       " -0.000531,\n",
       " -0.003634,\n",
       " -0.002289,\n",
       " -0.000626,\n",
       " 0.004089,\n",
       " -0.001688,\n",
       " 0.002588,\n",
       " 0.004141,\n",
       " 0.010842,\n",
       " 0.00107,\n",
       " 0.006035,\n",
       " -0.005503,\n",
       " 0.016965,\n",
       " -0.001544,\n",
       " 0.005332,\n",
       " 0.004329,\n",
       " -0.002427,\n",
       " 0.009539,\n",
       " 0.001806,\n",
       " 0.002386,\n",
       " 0.001555,\n",
       " -0.000354,\n",
       " 0.001823,\n",
       " -0.003668,\n",
       " 0.006647,\n",
       " 0.000238,\n",
       " 0.006995,\n",
       " -0.002169,\n",
       " 0.000718,\n",
       " -0.000288,\n",
       " -0.001508,\n",
       " 0.00395,\n",
       " -0.000166,\n",
       " -0.009835,\n",
       " 0.008755,\n",
       " -0.001973,\n",
       " -0.001789,\n",
       " -0.001014,\n",
       " -0.009136,\n",
       " -0.006873,\n",
       " -0.00546,\n",
       " -0.013583,\n",
       " -0.00226,\n",
       " -0.005945,\n",
       " 0.002665,\n",
       " 0.00933,\n",
       " 0.004062,\n",
       " -0.006291,\n",
       " 0.00214,\n",
       " 0.602485,\n",
       " 0.780518,\n",
       " -0.124505,\n",
       " 0.790516,\n",
       " -0.084429,\n",
       " -1.296883,\n",
       " 0.209746,\n",
       " 1.594077,\n",
       " 0.251495,\n",
       " -0.068748,\n",
       " 1.221134,\n",
       " -1.000455,\n",
       " -0.401872,\n",
       " 1.025712,\n",
       " -0.187153,\n",
       " 0.499948,\n",
       " -0.777534,\n",
       " 0.198912,\n",
       " 0.248203,\n",
       " 0.931615,\n",
       " 1.449783,\n",
       " 0.199454,\n",
       " -0.302099,\n",
       " 0.554541,\n",
       " -0.990163,\n",
       " 0.194181,\n",
       " -0.173485,\n",
       " 0.761199,\n",
       " 2.230305,\n",
       " 0.942009,\n",
       " 2.223557,\n",
       " -0.634311,\n",
       " -0.087829,\n",
       " -1.085626,\n",
       " -0.153923,\n",
       " -0.597421,\n",
       " -1.237392,\n",
       " 0.03585,\n",
       " 0.680259,\n",
       " 0.79998,\n",
       " 1.062276,\n",
       " 0.371227,\n",
       " 0.140654,\n",
       " -0.761164,\n",
       " 0.451356,\n",
       " 1.831193,\n",
       " 0.207598,\n",
       " -1.143447,\n",
       " -0.043554,\n",
       " -0.118031,\n",
       " -0.164068,\n",
       " 0.645296,\n",
       " 0.047641,\n",
       " 1.397986,\n",
       " 0.573546,\n",
       " -1.273272,\n",
       " 0.01547,\n",
       " -1.925581,\n",
       " 1.280917,\n",
       " 0.372514,\n",
       " 0.755584,\n",
       " 0.683558,\n",
       " 1.89531,\n",
       " 1.452825,\n",
       " -0.093403,\n",
       " -0.443367,\n",
       " -1.64694,\n",
       " -0.363296,\n",
       " -1.435261,\n",
       " -0.733192,\n",
       " -1.625476,\n",
       " -0.272702,\n",
       " -1.379663,\n",
       " -2.171799,\n",
       " 0.918889,\n",
       " -1.593586,\n",
       " -2.034975,\n",
       " 1.098133,\n",
       " -0.164919,\n",
       " -0.866094,\n",
       " -0.813101,\n",
       " -1.538368,\n",
       " -0.955199,\n",
       " -1.873132,\n",
       " 1.461812,\n",
       " -1.27104,\n",
       " -0.788613,\n",
       " 0.220811,\n",
       " -0.792705,\n",
       " -0.958521,\n",
       " 1.418126,\n",
       " 1.111518,\n",
       " 0.1875,\n",
       " 1.415105,\n",
       " 1.508498,\n",
       " -0.804211,\n",
       " -0.656693,\n",
       " 0.244515,\n",
       " 1.830848,\n",
       " 0.372199,\n",
       " -0.056567,\n",
       " 0.208371,\n",
       " -0.438346,\n",
       " -0.046675,\n",
       " 0.408777,\n",
       " -0.551978,\n",
       " -0.169341,\n",
       " 0.654716,\n",
       " -0.293343,\n",
       " -0.192323,\n",
       " -0.133401,\n",
       " 0.156716,\n",
       " 0.055189,\n",
       " -0.199032,\n",
       " -0.371858,\n",
       " -0.12471,\n",
       " -0.741537,\n",
       " 0.257637,\n",
       " -0.085806,\n",
       " 0.288506,\n",
       " 0.539942,\n",
       " -0.313288,\n",
       " -0.132998,\n",
       " -0.529974,\n",
       " -0.228006,\n",
       " 0.193563,\n",
       " -0.080768,\n",
       " 0.001026,\n",
       " 0.513109,\n",
       " 0.129193,\n",
       " 0.1238,\n",
       " -0.028533,\n",
       " 0.243007,\n",
       " -0.522251,\n",
       " -0.54148,\n",
       " -0.501779,\n",
       " -0.23493,\n",
       " -0.267986,\n",
       " -0.44273,\n",
       " -0.441263,\n",
       " 0.588567,\n",
       " -0.170506,\n",
       " 0.486674,\n",
       " 0.329826,\n",
       " 0.235379,\n",
       " 0.203148,\n",
       " -0.281664,\n",
       " 0.31809,\n",
       " -0.308685,\n",
       " 0.06798,\n",
       " -0.473664,\n",
       " -0.088361,\n",
       " 0.202828,\n",
       " 0.354156,\n",
       " -0.359619,\n",
       " -0.171873,\n",
       " -0.415299,\n",
       " -0.154442,\n",
       " 0.523797,\n",
       " -0.16851,\n",
       " -0.172284,\n",
       " 0.205491,\n",
       " 0.026909,\n",
       " 0.592122,\n",
       " -0.169825,\n",
       " 0.137128,\n",
       " -0.254854,\n",
       " 0.022945,\n",
       " -0.296439,\n",
       " -0.336005,\n",
       " -0.245296,\n",
       " -0.032754,\n",
       " -0.287965,\n",
       " -0.549065,\n",
       " 0.051082,\n",
       " -0.448761,\n",
       " -0.241221,\n",
       " 0.358847,\n",
       " -0.250688,\n",
       " 0.272592,\n",
       " -0.315513,\n",
       " -0.562226,\n",
       " -0.48383,\n",
       " -0.015125,\n",
       " -0.176865,\n",
       " -0.329488,\n",
       " 0.001285,\n",
       " 0.032,\n",
       " 0.307958,\n",
       " -0.817566,\n",
       " 0.281122,\n",
       " 0.479897,\n",
       " -0.121896,\n",
       " 0.443523,\n",
       " 0.174077,\n",
       " -0.301013,\n",
       " 0.166846,\n",
       " 0.266782,\n",
       " 0.041994,\n",
       " -0.456914,\n",
       " -0.227105,\n",
       " 0.245785,\n",
       " -0.750262,\n",
       " 0.085806,\n",
       " 0.800162,\n",
       " -0.932994,\n",
       " -0.65063,\n",
       " 1.183342,\n",
       " -0.45627,\n",
       " -0.130718,\n",
       " -0.256407,\n",
       " 0.285782,\n",
       " 0.257015,\n",
       " -0.414876,\n",
       " -0.793586,\n",
       " -0.098089,\n",
       " -1.124417,\n",
       " 0.581247,\n",
       " -0.169692,\n",
       " 0.507827,\n",
       " 0.946109,\n",
       " -0.532701,\n",
       " -0.273608,\n",
       " -1.015294,\n",
       " -0.328423,\n",
       " 0.218644,\n",
       " -0.25911,\n",
       " 0.043879,\n",
       " 0.856605,\n",
       " 0.245226,\n",
       " 0.164891,\n",
       " -0.194546,\n",
       " 0.401374,\n",
       " -0.745557,\n",
       " -0.798777,\n",
       " -0.802536,\n",
       " -0.536062,\n",
       " -0.614398,\n",
       " -0.753323,\n",
       " -0.804534,\n",
       " 0.950232,\n",
       " -0.521753,\n",
       " 0.887722,\n",
       " 0.74962,\n",
       " 0.296745,\n",
       " 0.351463,\n",
       " -0.412116,\n",
       " 0.702872,\n",
       " -0.684357,\n",
       " 0.285045,\n",
       " -0.733546,\n",
       " -0.358756,\n",
       " 0.458281,\n",
       " 0.707582,\n",
       " -0.6204,\n",
       " -0.39084,\n",
       " -0.866051,\n",
       " -0.129961,\n",
       " 0.902945,\n",
       " -0.307451,\n",
       " -0.237031,\n",
       " 0.542309,\n",
       " -0.099785,\n",
       " 0.682521,\n",
       " -0.341967,\n",
       " 0.201327,\n",
       " -0.439092,\n",
       " 0.051139,\n",
       " -0.378974,\n",
       " -0.717307,\n",
       " -0.365888,\n",
       " -0.069797,\n",
       " -0.336763,\n",
       " -0.82279,\n",
       " 0.178842,\n",
       " -0.926561,\n",
       " -0.478056,\n",
       " 0.517342,\n",
       " -0.373448,\n",
       " 0.524599,\n",
       " -0.570852,\n",
       " -0.971149,\n",
       " -0.961375,\n",
       " 0.161535,\n",
       " -0.332067,\n",
       " -0.568629,\n",
       " -0.140774,\n",
       " -0.22749,\n",
       " 0.549846,\n",
       " -1.427111,\n",
       " 0.357147,\n",
       " 0.702713,\n",
       " -0.295546,\n",
       " 0.838178,\n",
       " 0.277494,\n",
       " -0.541736,\n",
       " 0.21807,\n",
       " 0.254168,\n",
       " 0.03303,\n",
       " -0.921636,\n",
       " -0.253758,\n",
       " 0.219871,\n",
       " -0.975405,\n",
       " -0.138489,\n",
       " 0.737157,\n",
       " -0.864821,\n",
       " -0.622532,\n",
       " 0.664753,\n",
       " -0.654176,\n",
       " -0.40618,\n",
       " -0.275404,\n",
       " 0.219849,\n",
       " 0.215009,\n",
       " -0.725522,\n",
       " -0.502107,\n",
       " -0.084377,\n",
       " -0.814586,\n",
       " 0.038682,\n",
       " -0.163232,\n",
       " -0.031191,\n",
       " 0.993425,\n",
       " -0.726987,\n",
       " -0.061831,\n",
       " -0.989418,\n",
       " -0.940314,\n",
       " -0.062006,\n",
       " -0.336133,\n",
       " -0.184653,\n",
       " 0.635397,\n",
       " 0.379906,\n",
       " -0.104443,\n",
       " 0.395729,\n",
       " 0.327219,\n",
       " -0.793038,\n",
       " -1.000738,\n",
       " -0.9982,\n",
       " -0.878724,\n",
       " -0.255774,\n",
       " -0.910221,\n",
       " -0.517976,\n",
       " 1.324081,\n",
       " -0.7466,\n",
       " 0.501332,\n",
       " 0.941068,\n",
       " -0.002567,\n",
       " 0.17848,\n",
       " -0.631789,\n",
       " 0.562015,\n",
       " -0.611099,\n",
       " -0.237588,\n",
       " -0.75778,\n",
       " -0.6664,\n",
       " 0.551338,\n",
       " 0.312726,\n",
       " -0.560839,\n",
       " -0.319404,\n",
       " -0.997926,\n",
       " 0.090781,\n",
       " 0.856178,\n",
       " 0.222323,\n",
       " -0.076084,\n",
       " 0.066496,\n",
       " -0.070094,\n",
       " 0.985523,\n",
       " -0.229263,\n",
       " 0.644471,\n",
       " -0.13372,\n",
       " -0.364361,\n",
       " -0.357302,\n",
       " -1.10423,\n",
       " -0.311323,\n",
       " 0.04421,\n",
       " -0.969266,\n",
       " -0.725735,\n",
       " -0.18564,\n",
       " -1.261101,\n",
       " -0.165252,\n",
       " 0.754194,\n",
       " -0.661427,\n",
       " 0.175601,\n",
       " -0.859459,\n",
       " -0.502419,\n",
       " -0.85221,\n",
       " 0.21018,\n",
       " -0.398267,\n",
       " -0.252189,\n",
       " 0.114115,\n",
       " -0.386407,\n",
       " 0.971958,\n",
       " -1.797306,\n",
       " 0.45489,\n",
       " 0.780478,\n",
       " -0.182726,\n",
       " 0.427691,\n",
       " -0.039574,\n",
       " -0.831982,\n",
       " 0.19158,\n",
       " 0.295131,\n",
       " 0.284743,\n",
       " -0.884228,\n",
       " -0.070181,\n",
       " 0.041286,\n",
       " -0.283714,\n",
       " -0.110687,\n",
       " 0.270134,\n",
       " -0.286668,\n",
       " -0.249061,\n",
       " 0.260152,\n",
       " -0.13711,\n",
       " -0.053826,\n",
       " 0.039325,\n",
       " 0.105888,\n",
       " -0.016356,\n",
       " -0.194247,\n",
       " -0.192023,\n",
       " 0.027412,\n",
       " -0.266094,\n",
       " 0.007741,\n",
       " -0.106169,\n",
       " -0.003234,\n",
       " 0.316671,\n",
       " -0.211666,\n",
       " -0.017232,\n",
       " -0.227699,\n",
       " -0.188468,\n",
       " 0.022145,\n",
       " -0.022013,\n",
       " -0.004016,\n",
       " 0.231345,\n",
       " 0.14819,\n",
       " -0.025579,\n",
       " 0.135049,\n",
       " 0.084384,\n",
       " -0.163588,\n",
       " -0.259608,\n",
       " -0.339689,\n",
       " -0.216081,\n",
       " -0.076225,\n",
       " -0.269663,\n",
       " -0.168447,\n",
       " 0.414649,\n",
       " -0.150344,\n",
       " 0.286236,\n",
       " 0.249361,\n",
       " 0.088323,\n",
       " 0.114469,\n",
       " -0.140859,\n",
       " 0.159448,\n",
       " -0.219227,\n",
       " -0.023646,\n",
       " -0.221002,\n",
       " -0.119207,\n",
       " 0.213253,\n",
       " 0.134606,\n",
       " -0.221894,\n",
       " -0.068077,\n",
       " -0.258763,\n",
       " -0.033315,\n",
       " 0.207023,\n",
       " -0.054685,\n",
       " -0.143825,\n",
       " -0.061212,\n",
       " -0.024897,\n",
       " 0.193793,\n",
       " -0.062639,\n",
       " 0.079709,\n",
       " -0.127652,\n",
       " -0.019523,\n",
       " -0.213504,\n",
       " -0.300289,\n",
       " -0.063095,\n",
       " -0.002857,\n",
       " -0.31135,\n",
       " -0.248993,\n",
       " 0.028051,\n",
       " -0.419517,\n",
       " -0.134063,\n",
       " 0.094553,\n",
       " -0.128436,\n",
       " -0.00814,\n",
       " -0.254318,\n",
       " -0.104031,\n",
       " -0.247175,\n",
       " -0.002331,\n",
       " -0.092829,\n",
       " -0.149644,\n",
       " 0.056795,\n",
       " -0.115998,\n",
       " 0.254129,\n",
       " -0.589906,\n",
       " 0.154198,\n",
       " 0.29847,\n",
       " -0.02192,\n",
       " 0.215004,\n",
       " 0.114674,\n",
       " -0.239711,\n",
       " -0.034302,\n",
       " 0.122402,\n",
       " 0.163682,\n",
       " -0.193293,\n",
       " -0.143084,\n",
       " 0.307625,\n",
       " -0.924869,\n",
       " -0.149174,\n",
       " 0.335906,\n",
       " -0.793974,\n",
       " -0.235176,\n",
       " 0.579092,\n",
       " -0.360679,\n",
       " -0.542529,\n",
       " 0.255274,\n",
       " -0.094844,\n",
       " 0.093624,\n",
       " -0.555238,\n",
       " -0.331286,\n",
       " -0.020103,\n",
       " -0.561987,\n",
       " -0.240136,\n",
       " -0.224779,\n",
       " -0.142144,\n",
       " 0.891448,\n",
       " -0.823355,\n",
       " 0.30929,\n",
       " -0.710834,\n",
       " -0.831833,\n",
       " -0.051006,\n",
       " -0.324864,\n",
       " -0.346352,\n",
       " 0.684031,\n",
       " 0.733165,\n",
       " -0.294492,\n",
       " 0.472293,\n",
       " 0.375601,\n",
       " -0.615873,\n",
       " -0.623133,\n",
       " -1.060124,\n",
       " -0.618662,\n",
       " -0.03877,\n",
       " -0.481992,\n",
       " -0.290806,\n",
       " 1.172826,\n",
       " -0.323307,\n",
       " 0.207988,\n",
       " 0.657379,\n",
       " 0.108462,\n",
       " 0.306499,\n",
       " -0.669396,\n",
       " 0.124766,\n",
       " -0.357754,\n",
       " -0.394333,\n",
       " -0.298373,\n",
       " -0.071709,\n",
       " 0.324615,\n",
       " 0.178127,\n",
       " -0.351243,\n",
       " -0.38927,\n",
       " -0.664751,\n",
       " -0.273965,\n",
       " 0.537759,\n",
       " -0.059242,\n",
       " -0.36942,\n",
       " -0.417867,\n",
       " 0.314735,\n",
       " 0.775103,\n",
       " 0.019294,\n",
       " 0.462569,\n",
       " -0.129146,\n",
       " -0.478692,\n",
       " -0.583777,\n",
       " -1.060428,\n",
       " -0.528328,\n",
       " 0.170743,\n",
       " -0.957255,\n",
       " -0.638679,\n",
       " -0.105397,\n",
       " -1.238219,\n",
       " -0.36187,\n",
       " 0.558604,\n",
       " -0.428576,\n",
       " -0.312046,\n",
       " -0.797269,\n",
       " -0.086084,\n",
       " -0.437431,\n",
       " -0.473224,\n",
       " -0.170037,\n",
       " -0.375466,\n",
       " 0.001583,\n",
       " 0.130282,\n",
       " 0.779507,\n",
       " -1.323347,\n",
       " 0.451318,\n",
       " 0.74751,\n",
       " 0.088607,\n",
       " 0.120091,\n",
       " 0.022186,\n",
       " -0.725956,\n",
       " 0.091977,\n",
       " 0.18797,\n",
       " 0.758514,\n",
       " -0.265657,\n",
       " 0.086391,\n",
       " 0.612951,\n",
       " -1.555005,\n",
       " 0.067428,\n",
       " 0.288472,\n",
       " -1.425088,\n",
       " -0.220183,\n",
       " 0.750149,\n",
       " -0.610329,\n",
       " -0.801561,\n",
       " 0.284335,\n",
       " -0.477831,\n",
       " 0.298331,\n",
       " -0.909966,\n",
       " -0.392283,\n",
       " -0.144759,\n",
       " -1.192924,\n",
       " -0.133332,\n",
       " -0.483976,\n",
       " -0.363621,\n",
       " 1.296356,\n",
       " -1.306376,\n",
       " 0.374375,\n",
       " -1.285098,\n",
       " -1.296761,\n",
       " -0.171775,\n",
       " -0.524124,\n",
       " -0.65407,\n",
       " 1.17341,\n",
       " 0.876791,\n",
       " -0.258719,\n",
       " 0.38208,\n",
       " 0.639058,\n",
       " -1.014048,\n",
       " -0.878927,\n",
       " -1.46057,\n",
       " -0.967775,\n",
       " -0.256587,\n",
       " -0.563727,\n",
       " -0.643156,\n",
       " 1.726346,\n",
       " -0.359307,\n",
       " 0.083299,\n",
       " 1.103036,\n",
       " 0.234506,\n",
       " 0.38142,\n",
       " -1.173338,\n",
       " 0.104106,\n",
       " -0.391498,\n",
       " -0.551087,\n",
       " -0.686214,\n",
       " -0.087855,\n",
       " 0.430907,\n",
       " 0.100008,\n",
       " -0.703139,\n",
       " -0.64807,\n",
       " -1.017683,\n",
       " -0.19051,\n",
       " 0.923018,\n",
       " 0.275659,\n",
       " -0.360775,\n",
       " -0.516076,\n",
       " 0.588245,\n",
       " 1.472665,\n",
       " -0.235187,\n",
       " 0.72089,\n",
       " -0.126414,\n",
       " -0.636657,\n",
       " -0.688458,\n",
       " -1.412372,\n",
       " -0.885225,\n",
       " 0.548407,\n",
       " -1.349401,\n",
       " -1.1673,\n",
       " -0.371313,\n",
       " -1.545116,\n",
       " -0.382365,\n",
       " 1.118698,\n",
       " -0.634059,\n",
       " -0.363291,\n",
       " -1.161713,\n",
       " -0.315228,\n",
       " -0.592826,\n",
       " -0.443331,\n",
       " -0.329589,\n",
       " -0.773919,\n",
       " -0.215298,\n",
       " 0.362229,\n",
       " 1.249662,\n",
       " -1.653384,\n",
       " 0.765341,\n",
       " 1.300237,\n",
       " 0.245029,\n",
       " 0.29478,\n",
       " 0.133875,\n",
       " -1.054944,\n",
       " 0.255472,\n",
       " 0.293594,\n",
       " 0.988377,\n",
       " -0.576412,\n",
       " 0.153794,\n",
       " 0.176432,\n",
       " -1.008638,\n",
       " 0.300299,\n",
       " 0.102727,\n",
       " -1.381804,\n",
       " 0.187986,\n",
       " 1.64991,\n",
       " -0.373871,\n",
       " -0.257005,\n",
       " 0.308804,\n",
       " -0.367405,\n",
       " 0.035915,\n",
       " -0.099218,\n",
       " 0.105159,\n",
       " -0.054817,\n",
       " -1.026452,\n",
       " -0.331498,\n",
       " -0.119794,\n",
       " -0.354079,\n",
       " 1.458513,\n",
       " -0.95323,\n",
       " 0.809939,\n",
       " -0.905117,\n",
       " -0.506422,\n",
       " -0.343539,\n",
       " 0.306245,\n",
       " -0.297202,\n",
       " 1.099425,\n",
       " 0.611992,\n",
       " -0.660663,\n",
       " 0.606305,\n",
       " 0.878105,\n",
       " -0.312848,\n",
       " -0.913431,\n",
       " -0.943006,\n",
       " -0.19737,\n",
       " -0.272724,\n",
       " -0.486404,\n",
       " -0.603854,\n",
       " 1.230867,\n",
       " -0.501008,\n",
       " 0.69981,\n",
       " 0.874127,\n",
       " 0.318635,\n",
       " 0.952007,\n",
       " -0.404005,\n",
       " 0.347994,\n",
       " -0.706902,\n",
       " -0.386208,\n",
       " 0.313421,\n",
       " 0.774518,\n",
       " 0.551904,\n",
       " 0.304866,\n",
       " -0.881085,\n",
       " -0.206363,\n",
       " -0.98974,\n",
       " -1.034075,\n",
       " 0.928573,\n",
       " -0.764593,\n",
       " -0.821715,\n",
       " -0.603108,\n",
       " 0.024386,\n",
       " 1.183775,\n",
       " -0.37462,\n",
       " -0.190933,\n",
       " -0.260194,\n",
       " -0.063557,\n",
       " -0.562748,\n",
       " -1.082852,\n",
       " -0.933666,\n",
       " 0.274099,\n",
       " -0.875649,\n",
       " -0.766475,\n",
       " 0.495202,\n",
       " -1.366381,\n",
       " -1.015515,\n",
       " 1.054482,\n",
       " -0.880065,\n",
       " -0.666799,\n",
       " -0.555597,\n",
       " -0.501345,\n",
       " -0.363992,\n",
       " -1.48475,\n",
       " -0.081674,\n",
       " -0.155479,\n",
       " -0.242086,\n",
       " 0.707672,\n",
       " 0.48652,\n",
       " -1.580166,\n",
       " 0.440614,\n",
       " 0.884861,\n",
       " -0.521435,\n",
       " 0.356417,\n",
       " 0.491587,\n",
       " -0.451877,\n",
       " -0.227895,\n",
       " 0.56007,\n",
       " 1.242071,\n",
       " -0.002268,\n",
       " 0.338597,\n",
       " 0.58553,\n",
       " -1.440287,\n",
       " 0.667019,\n",
       " -0.080805,\n",
       " -1.514307,\n",
       " 0.149102,\n",
       " 1.176803,\n",
       " -0.634836,\n",
       " -0.517831,\n",
       " -0.184589,\n",
       " -0.475304,\n",
       " 0.452819,\n",
       " -0.691565,\n",
       " -0.350216,\n",
       " -0.409168,\n",
       " -1.605658,\n",
       " 0.453246,\n",
       " -0.482702,\n",
       " 0.159936,\n",
       " 1.197287,\n",
       " -1.113419,\n",
       " 0.412263,\n",
       " -1.432792,\n",
       " -0.738872,\n",
       " -0.232372,\n",
       " -0.440937,\n",
       " -0.719315,\n",
       " 1.181585,\n",
       " 0.346444,\n",
       " 0.153754,\n",
       " -0.291499,\n",
       " 0.720669,\n",
       " -1.153065,\n",
       " -0.828284,\n",
       " -0.874982,\n",
       " -0.45316,\n",
       " -0.571506,\n",
       " -0.374071,\n",
       " -1.083206,\n",
       " 1.119504,\n",
       " -0.413969,\n",
       " 0.169531,\n",
       " 1.23596,\n",
       " 0.104645,\n",
       " 0.318951,\n",
       " -0.956848,\n",
       " 0.1891,\n",
       " -0.385237,\n",
       " -0.083595,\n",
       " -0.261282,\n",
       " 0.486392,\n",
       " 0.010719,\n",
       " 0.208687,\n",
       " -0.501908,\n",
       " -0.826248,\n",
       " -0.780123,\n",
       " -0.296848,\n",
       " 1.005189,\n",
       " -0.200047,\n",
       " -0.436641,\n",
       " 0.106446,\n",
       " 0.455277,\n",
       " 1.340289,\n",
       " -0.466949,\n",
       " 0.597167,\n",
       " -0.242155,\n",
       " -0.183811,\n",
       " -0.5109,\n",
       " -0.970336,\n",
       " -0.90662,\n",
       " 0.429134,\n",
       " -0.47369,\n",
       " -1.123682,\n",
       " -0.233614,\n",
       " -0.670169,\n",
       " -0.370578,\n",
       " 1.203395,\n",
       " -1.007873,\n",
       " 0.012006,\n",
       " -0.578834,\n",
       " -1.189612,\n",
       " -0.644702,\n",
       " -0.664379,\n",
       " -0.067633,\n",
       " -0.773422,\n",
       " -0.617619,\n",
       " 0.794166,\n",
       " 0.410905,\n",
       " -0.968281,\n",
       " 0.512255,\n",
       " 0.957933,\n",
       " 0.06821,\n",
       " 0.238232,\n",
       " 0.438116,\n",
       " -0.572089,\n",
       " 0.495785,\n",
       " 0.037822,\n",
       " 0.35717,\n",
       " -0.80156]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_concat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_array = np.array(data_concat)\n",
    "np.random.shuffle(data_array)\n",
    "\n",
    "train = data_array[:60000]\n",
    "test = data_array[60000:80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "len(train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 20000 samples\n",
      "Epoch 1/100\n",
      "59850/60000 [============================>.] - ETA: 0s - loss: -4189.2157\n",
      "Epoch 00001: val_loss improved from inf to -4479.34883, saving model to model.h5\n",
      "60000/60000 [==============================] - 17s 288us/step - loss: -4190.0944 - val_loss: -4479.3488\n",
      "Epoch 2/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4546.2459\n",
      "Epoch 00002: val_loss improved from -4479.34883 to -4626.42594, saving model to model.h5\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: -4546.7003 - val_loss: -4626.4259\n",
      "Epoch 3/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4653.6747\n",
      "Epoch 00003: val_loss improved from -4626.42594 to -4713.03336, saving model to model.h5\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: -4653.4839 - val_loss: -4713.0334\n",
      "Epoch 4/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4712.7016\n",
      "Epoch 00004: val_loss improved from -4713.03336 to -4741.97794, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: -4712.6680 - val_loss: -4741.9779\n",
      "Epoch 5/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4751.8301\n",
      "Epoch 00005: val_loss improved from -4741.97794 to -4769.11376, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: -4752.7764 - val_loss: -4769.1138\n",
      "Epoch 6/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4781.2455\n",
      "Epoch 00006: val_loss improved from -4769.11376 to -4795.73766, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: -4781.0742 - val_loss: -4795.7377\n",
      "Epoch 7/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4802.6053\n",
      "Epoch 00007: val_loss improved from -4795.73766 to -4822.76080, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: -4802.5812 - val_loss: -4822.7608\n",
      "Epoch 8/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4819.9992\n",
      "Epoch 00008: val_loss improved from -4822.76080 to -4838.49087, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: -4819.7459 - val_loss: -4838.4909\n",
      "Epoch 9/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4833.2662\n",
      "Epoch 00009: val_loss improved from -4838.49087 to -4846.09815, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: -4832.7178 - val_loss: -4846.0982\n",
      "Epoch 10/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4843.1009\n",
      "Epoch 00010: val_loss improved from -4846.09815 to -4860.35232, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: -4843.5377 - val_loss: -4860.3523\n",
      "Epoch 11/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4854.2343\n",
      "Epoch 00011: val_loss improved from -4860.35232 to -4872.34486, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: -4854.0197 - val_loss: -4872.3449\n",
      "Epoch 12/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4863.7991\n",
      "Epoch 00012: val_loss improved from -4872.34486 to -4874.17577, saving model to model.h5\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: -4863.4231 - val_loss: -4874.1758\n",
      "Epoch 13/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4872.1560\n",
      "Epoch 00013: val_loss improved from -4874.17577 to -4890.78627, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: -4871.9247 - val_loss: -4890.7863\n",
      "Epoch 14/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4878.1074\n",
      "Epoch 00014: val_loss did not improve\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: -4877.9371 - val_loss: -4884.6315\n",
      "Epoch 15/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4882.7403\n",
      "Epoch 00015: val_loss did not improve\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: -4882.6177 - val_loss: -4882.7663\n",
      "Epoch 16/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4887.2768\n",
      "Epoch 00016: val_loss improved from -4890.78627 to -4901.21472, saving model to model.h5\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: -4887.1908 - val_loss: -4901.2147\n",
      "Epoch 17/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4891.6083\n",
      "Epoch 00017: val_loss did not improve\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: -4891.6508 - val_loss: -4901.1181\n",
      "Epoch 18/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4897.4788\n",
      "Epoch 00018: val_loss improved from -4901.21472 to -4911.34475, saving model to model.h5\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: -4897.1394 - val_loss: -4911.3448\n",
      "Epoch 19/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4901.5478\n",
      "Epoch 00019: val_loss improved from -4911.34475 to -4912.86772, saving model to model.h5\n",
      "60000/60000 [==============================] - 12s 207us/step - loss: -4901.1987 - val_loss: -4912.8677\n",
      "Epoch 20/100\n",
      "59550/60000 [============================>.] - ETA: 0s - loss: -4904.3223\n",
      "Epoch 00020: val_loss improved from -4912.86772 to -4920.17025, saving model to model.h5\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: -4904.2037 - val_loss: -4920.1702\n",
      "Epoch 21/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4905.8370\n",
      "Epoch 00021: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: -4906.5438 - val_loss: -4916.3137\n",
      "Epoch 22/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4909.3409\n",
      "Epoch 00022: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: -4909.0027 - val_loss: -4919.8605\n",
      "Epoch 23/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4910.5220\n",
      "Epoch 00023: val_loss improved from -4920.17025 to -4922.39329, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4910.8958 - val_loss: -4922.3933\n",
      "Epoch 24/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4912.8487\n",
      "Epoch 00024: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: -4913.0113 - val_loss: -4912.4910\n",
      "Epoch 25/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4915.3760\n",
      "Epoch 00025: val_loss improved from -4922.39329 to -4924.56896, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: -4915.1373 - val_loss: -4924.5690\n",
      "Epoch 26/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4917.0402\n",
      "Epoch 00026: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: -4916.7423 - val_loss: -4911.9127\n",
      "Epoch 27/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4918.3945\n",
      "Epoch 00027: val_loss improved from -4924.56896 to -4927.94828, saving model to model.h5\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: -4918.4397 - val_loss: -4927.9483\n",
      "Epoch 28/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4920.7858\n",
      "Epoch 00028: val_loss improved from -4927.94828 to -4932.24432, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4919.9775 - val_loss: -4932.2443\n",
      "Epoch 29/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4920.4302\n",
      "Epoch 00029: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: -4921.2837 - val_loss: -4927.6817\n",
      "Epoch 30/100\n",
      "59600/60000 [============================>.] - ETA: 0s - loss: -4922.5791\n",
      "Epoch 00030: val_loss improved from -4932.24432 to -4934.86230, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4922.9622 - val_loss: -4934.8623\n",
      "Epoch 31/100\n",
      "59550/60000 [============================>.] - ETA: 0s - loss: -4924.7729\n",
      "Epoch 00031: val_loss improved from -4934.86230 to -4938.74937, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: -4924.6197 - val_loss: -4938.7494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4926.7962\n",
      "Epoch 00032: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: -4926.4869 - val_loss: -4929.6345\n",
      "Epoch 33/100\n",
      "59850/60000 [============================>.] - ETA: 0s - loss: -4928.8307\n",
      "Epoch 00033: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: -4928.7720 - val_loss: -4936.9719\n",
      "Epoch 34/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4930.2240\n",
      "Epoch 00034: val_loss improved from -4938.74937 to -4944.85326, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: -4929.9757 - val_loss: -4944.8533\n",
      "Epoch 35/100\n",
      "59850/60000 [============================>.] - ETA: 0s - loss: -4931.2191\n",
      "Epoch 00035: val_loss did not improve\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: -4931.1220 - val_loss: -4941.7179\n",
      "Epoch 36/100\n",
      "59600/60000 [============================>.] - ETA: 0s - loss: -4932.7453\n",
      "Epoch 00036: val_loss improved from -4944.85326 to -4945.63788, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: -4932.5978 - val_loss: -4945.6379\n",
      "Epoch 37/100\n",
      "59850/60000 [============================>.] - ETA: 0s - loss: -4934.6675\n",
      "Epoch 00037: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4934.3260 - val_loss: -4941.8218\n",
      "Epoch 38/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4934.8203\n",
      "Epoch 00038: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: -4935.2109 - val_loss: -4943.8558\n",
      "Epoch 39/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4935.3772\n",
      "Epoch 00039: val_loss improved from -4945.63788 to -4950.54953, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: -4936.0761 - val_loss: -4950.5495\n",
      "Epoch 40/100\n",
      "59450/60000 [============================>.] - ETA: 0s - loss: -4937.4687\n",
      "Epoch 00040: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: -4938.0785 - val_loss: -4945.7324\n",
      "Epoch 41/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4940.8844\n",
      "Epoch 00041: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4940.6072 - val_loss: -4940.2738\n",
      "Epoch 42/100\n",
      "59550/60000 [============================>.] - ETA: 0s - loss: -4940.6555\n",
      "Epoch 00042: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: -4941.3917 - val_loss: -4941.3700\n",
      "Epoch 43/100\n",
      "59500/60000 [============================>.] - ETA: 0s - loss: -4942.3057\n",
      "Epoch 00043: val_loss did not improve\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: -4942.4206 - val_loss: -4947.9078\n",
      "Epoch 44/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4943.7582\n",
      "Epoch 00044: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: -4943.1702 - val_loss: -4949.6060\n",
      "Epoch 45/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4944.0040\n",
      "Epoch 00045: val_loss improved from -4950.54953 to -4956.53418, saving model to model.h5\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: -4943.8801 - val_loss: -4956.5342\n",
      "Epoch 46/100\n",
      "59450/60000 [============================>.] - ETA: 0s - loss: -4944.7036\n",
      "Epoch 00046: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: -4944.3702 - val_loss: -4954.7772\n",
      "Epoch 47/100\n",
      "59850/60000 [============================>.] - ETA: 0s - loss: -4945.7613\n",
      "Epoch 00047: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: -4945.1790 - val_loss: -4952.7591\n",
      "Epoch 48/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4945.6740\n",
      "Epoch 00048: val_loss improved from -4956.53418 to -4962.99668, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: -4945.6481 - val_loss: -4962.9967\n",
      "Epoch 49/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4946.0150\n",
      "Epoch 00049: val_loss did not improve\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: -4946.0964 - val_loss: -4948.6186\n",
      "Epoch 50/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4946.4842\n",
      "Epoch 00050: val_loss did not improve\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: -4946.7663 - val_loss: -4954.8783\n",
      "Epoch 51/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4947.8451\n",
      "Epoch 00051: val_loss did not improve\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: -4948.1191 - val_loss: -4954.5981\n",
      "Epoch 52/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4949.3191\n",
      "Epoch 00052: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: -4949.1431 - val_loss: -4959.9549\n",
      "Epoch 53/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4949.9969\n",
      "Epoch 00053: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: -4949.4977 - val_loss: -4960.3618\n",
      "Epoch 54/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4949.9963\n",
      "Epoch 00054: val_loss improved from -4962.99668 to -4963.83111, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: -4949.9959 - val_loss: -4963.8311\n",
      "Epoch 55/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4950.7001\n",
      "Epoch 00055: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: -4950.5218 - val_loss: -4958.8123\n",
      "Epoch 56/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4950.1137\n",
      "Epoch 00056: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: -4950.7992 - val_loss: -4954.1563\n",
      "Epoch 57/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4951.5758\n",
      "Epoch 00057: val_loss improved from -4963.83111 to -4966.21839, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4951.2769 - val_loss: -4966.2184\n",
      "Epoch 58/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4951.6231\n",
      "Epoch 00058: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4951.6296 - val_loss: -4948.2578\n",
      "Epoch 59/100\n",
      "59600/60000 [============================>.] - ETA: 0s - loss: -4952.7432\n",
      "Epoch 00059: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: -4952.0139 - val_loss: -4962.7940\n",
      "Epoch 60/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4952.3071\n",
      "Epoch 00060: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: -4952.6874 - val_loss: -4958.4594\n",
      "Epoch 61/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4953.4066\n",
      "Epoch 00061: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: -4953.3002 - val_loss: -4956.4178\n",
      "Epoch 62/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4955.9630\n",
      "Epoch 00062: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4956.0966 - val_loss: -4966.0125\n",
      "Epoch 63/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4956.8767\n",
      "Epoch 00063: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4957.0608 - val_loss: -4961.0623\n",
      "Epoch 64/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4958.6761\n",
      "Epoch 00064: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: -4957.8027 - val_loss: -4965.7961\n",
      "Epoch 65/100\n",
      "59850/60000 [============================>.] - ETA: 0s - loss: -4958.8358\n",
      "Epoch 00065: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: -4958.7485 - val_loss: -4962.4716\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59700/60000 [============================>.] - ETA: 0s - loss: -4957.8440\n",
      "Epoch 00066: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: -4959.0370 - val_loss: -4962.9457\n",
      "Epoch 67/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4960.2068\n",
      "Epoch 00067: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: -4959.5509 - val_loss: -4963.1348\n",
      "Epoch 68/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4959.9270\n",
      "Epoch 00068: val_loss improved from -4966.21839 to -4970.41985, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: -4960.3153 - val_loss: -4970.4199\n",
      "Epoch 69/100\n",
      "59600/60000 [============================>.] - ETA: 0s - loss: -4961.2962\n",
      "Epoch 00069: val_loss improved from -4970.41985 to -4974.52378, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: -4961.2603 - val_loss: -4974.5238\n",
      "Epoch 70/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4962.9042\n",
      "Epoch 00070: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: -4962.7360 - val_loss: -4974.4746\n",
      "Epoch 71/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4962.6141\n",
      "Epoch 00071: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: -4963.0417 - val_loss: -4947.5615\n",
      "Epoch 72/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4962.2522\n",
      "Epoch 00072: val_loss improved from -4974.52378 to -4974.61463, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: -4963.3756 - val_loss: -4974.6146\n",
      "Epoch 73/100\n",
      "59850/60000 [============================>.] - ETA: 0s - loss: -4963.9397\n",
      "Epoch 00073: val_loss improved from -4974.61463 to -4977.11056, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: -4963.9433 - val_loss: -4977.1106\n",
      "Epoch 74/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4964.6871\n",
      "Epoch 00074: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: -4964.1471 - val_loss: -4971.8650\n",
      "Epoch 75/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4964.4425\n",
      "Epoch 00075: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: -4964.3876 - val_loss: -4970.5527\n",
      "Epoch 76/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4964.7408\n",
      "Epoch 00076: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: -4964.4559 - val_loss: -4965.3143\n",
      "Epoch 77/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4964.9732\n",
      "Epoch 00077: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: -4965.2598 - val_loss: -4969.8237\n",
      "Epoch 78/100\n",
      "59600/60000 [============================>.] - ETA: 0s - loss: -4966.0701\n",
      "Epoch 00078: val_loss improved from -4977.11056 to -4977.39979, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: -4966.6191 - val_loss: -4977.3998\n",
      "Epoch 79/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4967.3413\n",
      "Epoch 00079: val_loss improved from -4977.39979 to -4980.31576, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: -4966.9403 - val_loss: -4980.3158\n",
      "Epoch 80/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4966.0122\n",
      "Epoch 00080: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: -4967.1797 - val_loss: -4972.4796\n",
      "Epoch 81/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4967.6562\n",
      "Epoch 00081: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: -4967.4715 - val_loss: -4976.9713\n",
      "Epoch 82/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4966.8623\n",
      "Epoch 00082: val_loss did not improve\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: -4967.9250 - val_loss: -4977.3112\n",
      "Epoch 83/100\n",
      "59850/60000 [============================>.] - ETA: 0s - loss: -4968.0797\n",
      "Epoch 00083: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: -4968.1497 - val_loss: -4972.3796\n",
      "Epoch 84/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4967.8608\n",
      "Epoch 00084: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: -4968.0796 - val_loss: -4970.6389\n",
      "Epoch 85/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4968.5610\n",
      "Epoch 00085: val_loss did not improve\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: -4968.4286 - val_loss: -4970.7387\n",
      "Epoch 86/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4967.5078\n",
      "Epoch 00086: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: -4968.3012 - val_loss: -4973.9900\n",
      "Epoch 87/100\n",
      "59650/60000 [============================>.] - ETA: 0s - loss: -4968.6239\n",
      "Epoch 00087: val_loss improved from -4980.31576 to -4983.81925, saving model to model.h5\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: -4968.6210 - val_loss: -4983.8192\n",
      "Epoch 88/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4968.0672\n",
      "Epoch 00088: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: -4968.6711 - val_loss: -4970.1069\n",
      "Epoch 89/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4968.3531\n",
      "Epoch 00089: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: -4968.7896 - val_loss: -4973.5607\n",
      "Epoch 90/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4969.0716\n",
      "Epoch 00090: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: -4969.1573 - val_loss: -4977.9848\n",
      "Epoch 91/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4968.5841\n",
      "Epoch 00091: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: -4969.1899 - val_loss: -4980.9343\n",
      "Epoch 92/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4969.7797\n",
      "Epoch 00092: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: -4970.0942 - val_loss: -4977.0150\n",
      "Epoch 93/100\n",
      "59900/60000 [============================>.] - ETA: 0s - loss: -4970.6263\n",
      "Epoch 00093: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: -4970.5487 - val_loss: -4981.1287\n",
      "Epoch 94/100\n",
      "59950/60000 [============================>.] - ETA: 0s - loss: -4970.8406\n",
      "Epoch 00094: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: -4970.5776 - val_loss: -4978.0330\n",
      "Epoch 95/100\n",
      "59700/60000 [============================>.] - ETA: 0s - loss: -4970.5810\n",
      "Epoch 00095: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: -4970.8619 - val_loss: -4975.7578\n",
      "Epoch 96/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4971.6048\n",
      "Epoch 00096: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: -4970.9070 - val_loss: -4983.3608\n",
      "Epoch 97/100\n",
      "59750/60000 [============================>.] - ETA: 0s - loss: -4970.5671\n",
      "Epoch 00097: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: -4970.9562 - val_loss: -4982.4404\n",
      "Epoch 98/100\n",
      "59850/60000 [============================>.] - ETA: 0s - loss: -4971.1647\n",
      "Epoch 00098: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: -4971.1199 - val_loss: -4981.7603\n",
      "Epoch 99/100\n",
      "59800/60000 [============================>.] - ETA: 0s - loss: -4970.8220\n",
      "Epoch 00099: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: -4971.2213 - val_loss: -4972.4553\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59950/60000 [============================>.] - ETA: 0s - loss: -4971.4444\n",
      "Epoch 00100: val_loss did not improve\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: -4971.3923 - val_loss: -4976.7031\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import h5py\n",
    "\n",
    "batch_size = 50\n",
    "original_dim = 1000\n",
    "latent_dim = 100\n",
    "intermediate_dim = 1200\n",
    "epochs = 100\n",
    "epsilon_std = 1.0\n",
    "\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "# Custom loss layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape:\n",
    "        return K.ones_like(x)\n",
    "\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "vae.compile(optimizer='rmsprop', loss=[zero_loss])\n",
    "\n",
    "#checkpoint\n",
    "cp = [keras.callbacks.ModelCheckpoint(filepath=\"model.h5\", verbose=1, save_best_only=True)]\n",
    "\n",
    "#train\n",
    "vae.fit(train, train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test, test), callbacks=cp)\n",
    "\n",
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# build a generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text From Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some matrix magic\n",
    "def sent_parse(sentence, mat_shape):\n",
    "    data_concat = []\n",
    "    word_vecs = vectorize_sentences(sentence)\n",
    "    for x in word_vecs:\n",
    "        data_concat.append(list(itertools.chain.from_iterable(x)))\n",
    "    zero_matr = np.zeros(mat_shape)\n",
    "    zero_matr[0] = np.array(data_concat)\n",
    "    return zero_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: original dimension sentence vector\n",
    "# output: text\n",
    "def print_sentence_with_w2v(sent_vect):\n",
    "    word_sent = ''\n",
    "    tocut = sent_vect\n",
    "    for i in range (int(len(sent_vect)/100)):\n",
    "        word_sent += w2v.most_similar(positive=[tocut[:100]], topn=1)[0][0]\n",
    "        word_sent += ' '\n",
    "        tocut = tocut[100:]\n",
    "    print(word_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: encoded sentence vector\n",
    "# output: encoded sentence vector in dataset with highest cosine similarity\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: two points, integer n\n",
    "# output: n equidistant points on the line between the input points (inclusive)\n",
    "def shortest_homology(point_one, point_two, num):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = True)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: two written sentences, VAE batch-size, dimension of VAE input\n",
    "# output: the function embeds the sentences in latent-space, and then prints their generated text representations\n",
    "# along with the text representations of several points in between them\n",
    "def sent_2_sent(sent1,sent2, batch, dim):\n",
    "    a = sent_parse([sent1], (batch,dim))\n",
    "    b = sent_parse([sent2], (batch,dim))\n",
    "    encode_a = encoder.predict(a, batch_size = batch)\n",
    "    encode_b = encoder.predict(b, batch_size = batch)\n",
    "    test_hom = shortest_homology(encode_a[0], encode_b[0], 5)\n",
    "    \n",
    "    for point in test_hom:\n",
    "        p = generator.predict(np.array([point]))[0]\n",
    "        print_sentence_with_w2v(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing sentences from the training set and comparing them with the original will test whether the custom print function works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy cached user credentials from web browsers and chat clients \n",
      "actors left phpinfo.php script on the webserver running on port \n",
      "limited webfacing used fictitious this infected workflow 104.238.184.252 Cisco3045EK9 fictitious \n",
      "revealed webfacing used fictitious the webfacing Bit9signed provided Cisco3045EK9 fictitious \n",
      "limited fictitious Whether fictitious available webfacing nonsensitive Webbased Cisco3045EK9 Webbased \n",
      "provided on Launches main analysis webfacing nonsensitive Webbased Cisco3045EK9 over \n",
      "obtained on Launches following . cyberterrorists •httpwww.lemonde.frpixelsarticle20150409lessitesdetv5mondedetournesparungroupe Webbased stolen over \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_sentence_with_w2v(train[15])\n",
    "print_sentence_with_w2v(train[20])\n",
    "sent_2_sent('already legitimately used in an Internet Explorer plugin open source','depending on Windows version . Returns failure or success with',batch=50,dim=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes the training set of sentence vectors (concatenanted word vectors) and embeds them into a lower dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_encoded = encoder.predict(np.array(train), batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder takes the list of latent dimensional encodings from above and turns them back into vectors of their original dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1000)\n"
     ]
    }
   ],
   "source": [
    "sent_decoded = generator.predict(sent_encoded)\n",
    "print(sent_decoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder trained above embeds sentences (concatenated word vetors) into a lower dimensional space. The code below takes two of these lower dimensional sentence representations and finds five points between them. It then uses the trained decoder to project these five points into the higher, original, dimensional space. Finally, it reveals the text represented by the five generated sentence vectors by taking each word vector concatenated inside and finding the text associated with it in the word2vec used during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devised 4e84b1448cf96fabe88c623b222057c4 cyberterrorists families relayed crossplatform different methods Classic 0BCEh—webcam \n",
      "webfacing 4e84b1448cf96fabe88c623b222057c4 pwNative.exe families relayed crossplatform different methods Classic 104.238.184.252 \n",
      "webfacing 4e84b1448cf96fabe88c623b222057c4 month.30 families different further different different Classic 104.238.184.252 \n",
      "webfacing Battery month.30 different different traits different different Classic •httpwww.lemonde.frpixelsarticle20150409lessitesdetv5mondedetournesparungroupe \n",
      "webfacing update.msntoole.com Sakula different different common different different wiper webfacing \n",
      "compromised webfacing primary different different common different different 104.238.184.252 webfacing \n",
      "infected Russiabased . relayed different common filtered different 104.238.184.252 webfacing \n",
      "infected Russiabased . relayed different common filtered relayed webfacing webfacing \n",
      "infected government . relayed different common filtered relayed webfacing webfacing \n",
      "infected government . relayed different common filtered relayed webfacing webfacing \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_hom = shortest_homology(sent_encoded[105], sent_encoded[107], 10)\n",
    "for point in test_hom:\n",
    "    p = generator.predict(np.array([point]))[0]\n",
    "    print_sentence_with_w2v(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does the same thing, with one important difference. After sampling equidistant points in the latent space between two sentence embeddings, it finds the embeddings from our encoded dataset those points are most similar to. It then prints the text associated with those vectors.\n",
    "  \n",
    "This allows us to explore how the Variational Autoencoder clusters our dataset of sentences in latent space. It lets us investigate whether sentences with similar concepts or grammatical styles are represented in similar areas of the lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precise threat targeted fictitious the familyBackdoor.Winnti devised fictitious new signiﬁcant \n",
      "precise threat targeted fictitious the familyBackdoor.Winnti devised fictitious new signiﬁcant \n",
      "precise threat targeted fictitious the familyBackdoor.Winnti devised fictitious new signiﬁcant \n",
      "infected a infected fictitious the webfacing Bit9signed of the Bit9signed \n",
      "infected a infected fictitious the webfacing Bit9signed of the Bit9signed \n",
      "memorystorage a new memorystorage its Bit9signed webfacing Bit9signed the different \n",
      "memorystorage a new memorystorage its Bit9signed webfacing Bit9signed the different \n",
      "memorystorage a new memorystorage its Bit9signed webfacing Bit9signed the different \n",
      "memorystorage a new memorystorage its Bit9signed webfacing Bit9signed the different \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-02e0a0ad52fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_hom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshortest_homology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpoint\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_hom\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfind_similar_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint_sentence_with_w2v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6a917b28af24>\u001b[0m in \u001b[0;36mfind_similar_encoding\u001b[1;34m(sent_vect)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mall_cosine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_encoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_vect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mall_cosine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdata_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_cosine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mcosine\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[1;31m# cosine distance is also referred to as 'uncentered correlation',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[1;31m#   or 'reflective correlation'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[1;34m(u, v, w, centered)\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[0muv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[0muu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m     \u001b[0mvv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m     \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0muv\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muu\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1128\u001b[1;33m         \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1129\u001b[0m         \u001b[0mscl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_hom = shortest_homology(sent_encoded[2], sent_encoded[1500], 20)\n",
    "for point in test_hom:\n",
    "    p = generator.predict(np.array([find_similar_encoding(point)]))[0]\n",
    "    print_sentence_with_w2v(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
