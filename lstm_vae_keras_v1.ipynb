{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import objectives, backend as K\n",
    "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "import numpy as np\n",
    "import uuid\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle\n",
    "import itertools\n",
    "import mycallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VAE(object):\n",
    "    \n",
    "    def _build_decoder(self, encoded, vocab_size, max_length):\n",
    "        repeated_context = RepeatVector(max_length)(encoded)\n",
    "\n",
    "        #h = LSTM(1000, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "        h = LSTM(1000, return_sequences=True, name='dec_lstm_2')(repeated_context)\n",
    "\n",
    "        decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)\n",
    "\n",
    "        return decoded\n",
    "    def _build_encoder(self, x, latent_rep_size=100, max_length=300, epsilon_std=0.01):\n",
    "        #h = Bidirectional(LSTM(1000, return_sequences=True, name='lstm_1'), merge_mode='concat')(x)\n",
    "        h = Bidirectional(LSTM(1000, return_sequences=False, name='lstm_2'), merge_mode='concat')(x)\n",
    "        h = Dense(435, activation='tanh', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev=epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation='linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation='linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "\n",
    "    def create(self, vocab_size=1000, max_length=50, latent_rep_size=100):\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.autoencoder = None\n",
    "\n",
    "        x = Input(shape=(max_length,))\n",
    "        x_embed = Embedding(vocab_size, 100, input_length=max_length)(x)\n",
    "\n",
    "        vae_loss, encoded = self._build_encoder(x_embed, latent_rep_size=latent_rep_size, max_length=max_length)\n",
    "        self.encoder = Model(inputs=x, outputs=encoded)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "\n",
    "\n",
    "        decoded = self._build_decoder(encoded_input, vocab_size, max_length)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "\n",
    "        self.autoencoder = Model(inputs=x, outputs=[self._build_decoder(encoded, vocab_size, max_length)])\n",
    "        adam=keras.optimizers.Adam(lr=.001)\n",
    "        self.autoencoder.compile(optimizer=adam,\n",
    "                                 loss=[vae_loss],\n",
    "                                 metrics=['accuracy'])\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_WORDS=500\n",
    "MAX_LENGTH=15\n",
    "VALIDATION_SPLIT =.3\n",
    "_EOS = \"endofsent\"\n",
    "\n",
    "def sent_parse(sentences,tokenizer=None,build_indices=True):\n",
    "    if build_indices:\n",
    "        tokenizer = Tokenizer(nb_words=NUM_WORDS)\n",
    "        tokenizer.fit_on_texts(sentences)\n",
    "        sequences = tokenizer.texts_to_sequences(sentences)\n",
    "        word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(word_index))\n",
    "        data = pad_sequences(sequences, maxlen=MAX_LENGTH)\n",
    "        indices = np.arange(data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        data = data[indices]\n",
    "    else:\n",
    "        sequences = tokenizer.texts_to_sequences(sentences)\n",
    "        data = pad_sequences(sequences, maxlen=MAX_LENGTH)\n",
    "        indices = np.arange(data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        data = data[indices]\n",
    "    return tokenizer,data\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec\n",
    "\n",
    "\n",
    "def interpolate_b_points(point_one, point_two, num,useSperical=True):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = False)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        if useSperical:\n",
    "            hom_sample.append(slerp(s,point_one,point_two))\n",
    "        else:\n",
    "            hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample\n",
    "\n",
    "\n",
    "def sent_2_sent(sent1,sent2, model,tokenizer=None):\n",
    "    _,a = sent_parse([sent1],tokenizer,build_indices=False)\n",
    "    _,b = sent_parse([sent2],tokenizer,build_indices=False)\n",
    "    encode_a = model.encoder.predict(a)\n",
    "    encode_b = model.encoder.predict(b)\n",
    "    test_hom = interpolate_b_points(encode_a, encode_b, 5,False)\n",
    "    index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "    for point in test_hom:\n",
    "        words=[]\n",
    "        deco=model.decoder.predict(point)\n",
    "        #print(deco)\n",
    "        for seq in deco[0]:\n",
    "            words.append(index_word[np.argmax(seq)])\n",
    "            words.append(' ')\n",
    "        print(''.join(words))\n",
    "        \n",
    "       \n",
    "def slerp(val, low, high):\n",
    "    \"\"\"Spherical interpolation. val has a range of 0 to 1.\"\"\"\n",
    "    if val <= 0:\n",
    "        return low\n",
    "    elif val >= 1:\n",
    "        return high\n",
    "    omega = np.arccos(np.dot(low/np.linalg.norm(low), high/np.linalg.norm(high)))\n",
    "    so = np.sin(omega)\n",
    "    return np.sin((1.0-val)*omega) / so * low + np.sin(val*omega)/so * high\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540093\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "def split_into_sent (text):\n",
    "    strg = ''\n",
    "    for word in text:\n",
    "        strg += word\n",
    "        strg += ' '\n",
    "    strg_cleaned = strg.lower()\n",
    "    for x in ['\\n','\"',\"!\", '#','$','%','&','(',')','*','+',',','-','/',':',';','<','=','>','?','@','[','^',']','_','`','{','|','}','~','\\t']:\n",
    "        strg_cleaned = strg_cleaned.replace(x, '')\n",
    "    sentences = sent_tokenize(strg_cleaned)\n",
    "    return sentences\n",
    "\n",
    "fiction_text=brown.words(categories=['fiction','humor', 'learned', 'lore', 'mystery', 'news'])\n",
    "print(len(fiction_text))\n",
    "sents=sent_tokenize(' '.join(fiction_text))[1:10000]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 121030 unique tokens.\n",
      "Training data\n",
      "(63904, 15)\n",
      "Number of words:\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "###### APT Text#############\n",
    "\n",
    "import nltk\n",
    "import codecs\n",
    "with codecs.open('/home/vmangipudi/APT_reports_combined.txt',\"r\",encoding=\"utf-8\") as f:\n",
    "    fl=f.readlines()\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=''.join(fl).replace('\\n',' ')\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer,data=sent_parse(sents)\n",
    "\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "X_train = data[:-nb_validation_samples]\n",
    "X_test = data[-nb_validation_samples:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training data\")\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"Number of words:\")\n",
    "print(len(np.unique(np.hstack(X_train))))\n",
    "\n",
    "\n",
    "\n",
    "temp = np.zeros((X_train.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_train.shape[0]), axis=0).reshape(X_train.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_train.shape[0], axis=0), X_train] = 1\n",
    "\n",
    "X_train_one_hot = temp\n",
    "\n",
    "temp = np.zeros((X_test.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_test.shape[0]), axis=0).reshape(X_test.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_test.shape[0], axis=0), X_test] = 1\n",
    "\n",
    "x_test_one_hot = temp\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + \\\n",
    "                model_name\n",
    "               #model_name + \"-{epoch:02d}-{val_decoded_mean_acc:.2f}-{val_pred_loss:.2f}.h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "    \n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath,\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False)\n",
    "\n",
    "    return checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "def train():\n",
    "    model = VAE()\n",
    "    model.create(vocab_size=NUM_WORDS, max_length=MAX_LENGTH)\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "    checkpointer = create_model_checkpoint('models', 'rnn_ae')\n",
    "    cb=mycallback.validate_after_epoch(model,tokenizer,sents)\n",
    "    \n",
    "    \n",
    "    model.autoencoder.fit(x=X_train, y={'decoded_mean': X_train_one_hot},\n",
    "                          batch_size=500, epochs=150, callbacks=[checkpointer,tensorboard,cb],\n",
    "                          validation_data=(X_test, {'decoded_mean': x_test_one_hot}),shuffle=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63904 samples, validate on 27387 samples\n",
      "Epoch 1/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.4199Epoch 00001: saving model to models/rnn_ae\n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "63904/63904 [==============================] - 32s 494us/step - loss: 0.1401 - acc: 0.4199 - val_loss: 0.1205 - val_acc: 0.4378\n",
      "Epoch 2/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.4423Epoch 00002: saving model to models/rnn_ae\n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "63904/63904 [==============================] - 34s 537us/step - loss: 0.1185 - acc: 0.4422 - val_loss: 0.1164 - val_acc: 0.4436\n",
      "Epoch 3/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.4491Epoch 00003: saving model to models/rnn_ae\n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "63904/63904 [==============================] - 32s 508us/step - loss: 0.1149 - acc: 0.4490 - val_loss: 0.1135 - val_acc: 0.4278\n",
      "Epoch 4/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.4535Epoch 00004: saving model to models/rnn_ae\n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "63904/63904 [==============================] - 33s 513us/step - loss: 0.1116 - acc: 0.4535 - val_loss: 0.1094 - val_acc: 0.4534\n",
      "Epoch 5/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.4600Epoch 00005: saving model to models/rnn_ae\n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "63904/63904 [==============================] - 33s 513us/step - loss: 0.1080 - acc: 0.4599 - val_loss: 0.1123 - val_acc: 0.4544\n",
      "Epoch 6/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.4650Epoch 00006: saving model to models/rnn_ae\n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "63904/63904 [==============================] - 33s 510us/step - loss: 0.1059 - acc: 0.4650 - val_loss: 0.1048 - val_acc: 0.4655\n",
      "Epoch 7/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.4684Epoch 00007: saving model to models/rnn_ae\n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "function function function function function function function function function function function function function function function \n",
      "63904/63904 [==============================] - 33s 510us/step - loss: 0.1034 - acc: 0.4684 - val_loss: 0.1030 - val_acc: 0.4681\n",
      "Epoch 8/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.4803Epoch 00008: saving model to models/rnn_ae\n",
      "may may may may may may may may may may may may may may may \n",
      "may may may may may function function function function function function function function function function \n",
      "back function function function function function function function function function function function function function function \n",
      "first first function function function function function function function function function function function function function \n",
      "first first first first sample sample sample sample sample sample sample sample sample sample sample \n",
      "63904/63904 [==============================] - 31s 492us/step - loss: 0.0993 - acc: 0.4804 - val_loss: 0.0970 - val_acc: 0.4846\n",
      "Epoch 9/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.4934Epoch 00009: saving model to models/rnn_ae\n",
      "include include 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 \n",
      "service service service service service service service service service service service function function function function \n",
      "back back back back back back back back back back back back back back back \n",
      "back back back back back back back back back back back back back back back \n",
      "back back back back back back back back back back would would would would would \n",
      "63904/63904 [==============================] - 33s 518us/step - loss: 0.0946 - acc: 0.4935 - val_loss: 0.0957 - val_acc: 0.4853\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.5124Epoch 00010: saving model to models/rnn_ae\n",
      "may may so so so so so so so so so so so so so \n",
      "service service service service service service so so so so so so so so so \n",
      "back back back back service service service service service service service service service service service \n",
      "back back back back back back back back back back back back back back back \n",
      "back back back back back back back back back back sample sample sample sample sample \n",
      "63904/63904 [==============================] - 33s 518us/step - loss: 0.0898 - acc: 0.5123 - val_loss: 0.0904 - val_acc: 0.5099\n",
      "Epoch 11/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.5339Epoch 00011: saving model to models/rnn_ae\n",
      "include actor actor actor actor actor actor actor actor actor actor actor actor actor actor \n",
      "back details details details details actor actor actor actor actor actor actor actor actor actor \n",
      "back back back back back back back back back actor actor actor actor actor actor \n",
      "back back back back back back back dll dll dll dll dll dll dll dll \n",
      "back back dll dll dll dll dll dll dll dll dll dll dll dll dll \n",
      "63904/63904 [==============================] - 33s 518us/step - loss: 0.0852 - acc: 0.5340 - val_loss: 0.0824 - val_acc: 0.5470\n",
      "Epoch 12/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.5662Epoch 00012: saving model to models/rnn_ae\n",
      "module module module being being module module module module out out out out out out \n",
      "module module module module module module out out out out out out out out out \n",
      "05 05 05 05 05 05 05 named named named named named named named out \n",
      "05 05 05 05 05 05 05 05 05 sample sample sample sample sample sample \n",
      "05 05 05 05 05 05 05 sample sample sample sample sample sample sample sample \n",
      "63904/63904 [==============================] - 33s 516us/step - loss: 0.0781 - acc: 0.5663 - val_loss: 0.0756 - val_acc: 0.5804\n",
      "Epoch 13/150\n",
      "63500/63904 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.6007Epoch 00013: saving model to models/rnn_ae\n",
      "being being being being being being being being being being being being being being being \n",
      "website being being being being being being being being being being being being being being \n",
      "website website website last sample sample sample sample sample sample sample sample sample sample sample \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ceb2441fc49c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-8ad52ff25ecc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     model.autoencoder.fit(x=X_train, y={'decoded_mean': X_train_one_hot},\n\u001b[1;32m     13\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                           validation_data=(X_test, {'decoded_mean': x_test_one_hot}),shuffle=True)\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vmangipudi/SentenceVAE/mycallback.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0msent1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Last year, malware which  purported to be the Tor Browser Bundle was found in the wild.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0msent2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'It was found to be backdoored by Gh0st RAT  and exfiltrated data to an IP in China.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0msent_2_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vmangipudi/SentenceVAE/mycallback.py\u001b[0m in \u001b[0;36msent_2_sent\u001b[0;34m(sent1, sent2, model, tokenizer)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m#print(deco)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "model=train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,100) and (1,100) not aligned: 100 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-77de430c12b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent_2_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m210\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m215\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-9c6682d0c764>\u001b[0m in \u001b[0;36msent_2_sent\u001b[0;34m(sent1, sent2, model, tokenizer)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mencode_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mencode_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtest_hom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolate_b_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mindex_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-9c6682d0c764>\u001b[0m in \u001b[0;36minterpolate_b_points\u001b[0;34m(point_one, point_two, num, useSperical)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museSperical\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mhom_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslerp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpoint_one\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpoint_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mhom_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_one\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdist_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-9c6682d0c764>\u001b[0m in \u001b[0;36mslerp\u001b[0;34m(val, low, high)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0momega\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marccos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momega\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0momega\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mso\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0momega\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mso\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,100) and (1,100) not aligned: 100 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "sent_2_sent(sents[210],sents[215],model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'It was found to be backdoored by Gh0st RAT  and exfiltrated data to an IP in China.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown loss function:vae_loss",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9e777c7d3138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/vmangipudi/SentenceVAE/models/rnn_ae'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    269\u001b[0m                       \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                       \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                       sample_weight_mode=sample_weight_mode)\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Set optimizer weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m                                  \u001b[0;34m' outputs, but you passed loss='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                                  str(loss))\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/keras/losses.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/keras/losses.pyc\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m    101\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                                     printable_module_name='loss function')\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 160\u001b[0;31m                                  ':' + function_name)\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown loss function:vae_loss"
     ]
    }
   ],
   "source": [
    "keras.models.load_model('/home/vmangipudi/SentenceVAE/models/rnn_ae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deco=model.decoder.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(deco[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
